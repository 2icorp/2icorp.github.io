---
layout: default
title: "FLUX.1-schnell의 Distillation 제거와 Fine-tuning 방법: 전문가 가이드"
date: 2024-10-21
categories: AI, Machine Learning
tags: [Pytorch, FLUX, Distillation, Fine-tuning]
author: 한효정
---

# FLUX.1-schnell의 Distillation 제거와 Fine-tuning 방법: 전문가 가이드

**FLUX.1-schnell** 모델은 뛰어난 성능을 가진 텍스트-이미지 생성 모델로, 빠른 이미지 생성을 위해 **distillation**이 적용되었습니다. 그러나 distillation된 모델은 일부 중요한 기능이 제거되어 fine-tuning이 어렵거나 불가능한 경우가 있습니다. 이 글에서는 **distillation**을 제거하고 **fine-tuning**을 통해 모델의 성능을 복원하고, 더 창의적이고 유연한 이미지 생성을 가능하게 하는 방법을 설명하겠습니다.

## 1. Distillation이란?

**Distillation**은 큰 모델(Teacher 모델)이 학습한 지식을 작은 모델(Student 모델)로 압축하는 기술입니다. 이를 통해 더 작은 모델이 메모리나 계산 자원을 절약하면서도 빠르게 예측을 할 수 있습니다. 하지만 distillation 과정에서 일부 기능이나 성능이 손실될 수 있습니다. 특히 **Classifier-Free Guidance(CFG)**와 같은 기능이 제거될 수 있으며, 이는 텍스트-이미지 생성에서 중요한 역할을 합니다.

### Distillation의 장단점
- **장점**: 계산 속도가 빨라지고, 메모리 사용이 줄어듦.
- **단점**: 일부 중요한 기능이 제거되어 fine-tuning이 제한적일 수 있음.

## 2. FLUX.1-schnell의 Distillation 해제

**FLUX.1-schnell**은 distillation된 상태로 제공되어 빠른 이미지 생성을 가능하게 하지만, fine-tuning이 불가능하다는 한계가 있습니다. 이를 극복하기 위해 distillation을 제거하고, 원래의 **Teacher 모델**의 학습 목표를 복원하는 방법을 사용할 수 있습니다.

### Distillation 해제 방법
1. **원래 학습 목표 복원**: FLUX.1-schnell의 원래 학습 목표인 **flow-matching objective**와 **MSE 손실 함수**를 사용하여 모델을 재학습시킵니다. 이를 통해 distillation 과정에서 손실된 성능을 회복할 수 있습니다.
2. **Classifier-Free Guidance 복구**: distillation 과정에서 제거된 **CFG 기능**을 다시 복원하여 조건부(프롬프트)와 비조건부 이미지 생성을 동시에 학습할 수 있게 합니다.

3. **어텐션 마스킹(Attention Masking)**:
   - FLUX.1-schnell은 텍스트 인코더로 **T5-XXL**을 사용합니다. 이때 텍스트 길이를 256 또는 512 토큰으로 패딩(padding)하는 방식으로 작동합니다. 문제는 패딩된 토큰이 불필요한 정보를 포함할 수 있다는 점입니다.
   - 이를 방지하기 위해, 패딩된 토큰을 **마스킹(masking)** 처리하여 주어진 프롬프트에서 불필요한 정보가 이미지 생성에 반영되지 않도록 해야 합니다.

4. **Low-rank Fine-tuning**:
   - 모델의 파라미터 수가 매우 많기 때문에, 모든 파라미터를 사용하는 **풀 랭크(full-rank) fine-tuning**은 GPU 자원을 많이 소모합니다. 이를 피하기 위해 **LoRA**(Low-Rank Adaptation) 또는 **LoKr**와 같은 **low-rank** 기반 fine-tuning 기법을 사용해 작은 모델에서도 효과적으로 학습할 수 있도록 합니다.
   - LoKr(Locally Rank-K Approximation)은 **low-rank** 기법으로 distillation된 모델의 성능을 개선하기 위해 사용됩니다. 이 방법은 모델의 모든 파라미터를 조정하지 않고 일부 파라미터만 조정해 학습 속도를 높입니다.

5. **Loss 함수 설정**:
   - distillation을 해제하고 fine-tune하는 과정에서 **손실 함수(loss function)**의 설정이 중요합니다. **MSE 손실 함수**는 모델의 예측값과 실제값 간의 차이를 최소화하는 역할을 합니다. 이 손실 함수는 distillation된 모델을 다시 학습시켜 원래의 성능을 회복하게 도와줍니다.

6. **Temperature Scaling**:
   - distillation 해제 과정에서 **Temperature Scaling** 기법을 적용하여 모델의 출력을 부드럽게 만들 수 있습니다. 이를 통해 이미지 생성의 다양성과 창의성을 높일 수 있습니다.

## 3. 실제 모델 예시: OpenFLUX.1과 LibreFLUX

두 개의 모델은 FLUX.1-schnell의 distillation을 제거하고 fine-tuning된 결과로 탄생했습니다. 각각의 특징을 살펴보겠습니다.

### [OpenFLUX.1](https://huggingface.co/ostris/OpenFLUX.1)

**OpenFLUX.1**은 FLUX.1-schnell에서 distillation을 제거하고 오픈 소스화된 모델입니다. 이 모델은 더 유연한 fine-tuning이 가능하며, 상업적으로도 자유롭게 사용할 수 있습니다. OpenFLUX.1은 Apache 2.0 라이선스 하에 배포되어 누구나 수정하고 사용할 수 있으며, distillation을 제거하여 원래 모델의 학습 목표와 성능을 회복했습니다.

주요 특징:
- Apache 2.0 라이선스
- Distillation 해제 및 원래 학습 목표 복원
- 상업적 사용 가능

### [LibreFLUX](https://huggingface.co/jimmycarter/LibreFLUX)

**LibreFLUX**는 FLUX.1-schnell의 distillation을 제거하고 fine-tuning한 또 다른 모델입니다. LibreFLUX는 특히 **attention masking**을 적용하여 패딩 토큰으로 인한 성능 저하를 방지하였고, 더 긴 텍스트 시퀀스를 처리할 수 있도록 개선되었습니다. 이 모델은 상업적 용도로도 사용할 수 있으며, 다양한 데이터 분포에 맞게 쉽게 fine-tuning할 수 있는 장점을 가지고 있습니다.

주요 특징:
- Attention Masking 적용
- 상업적 사용 가능
- 더 긴 텍스트 시퀀스 처리 가능

## 4. 결론

FLUX.1-schnell 모델은 빠른 성능을 제공하는 distillation된 모델이지만, distillation 해제를 통해 더 유연하고 강력한 모델로 발전시킬 수 있습니다. OpenFLUX.1과 LibreFLUX는 distillation을 제거하고 fine-tuning한 모델로, 원래 FLUX 모델의 성능을 복구하고 창의적이고 다양한 이미지 생성을 가능하게 합니다. 이를 통해 더 많은 사용자가 자신만의 데이터 분포에 맞게 모델을 fine-tune하여 사용할 수 있게 되었습니다.

---

## 최신 글
<ul>
  {% for post in site.posts %}
    <li>
      <a href="{{ post.url }}">{{ post.title }}</a>
      <span>{{ post.date | date: "%B %d, %Y" }}</span>
    </li>
  {% endfor %}
</ul>


