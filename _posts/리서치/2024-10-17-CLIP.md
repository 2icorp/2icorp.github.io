---
layout: default
title: "CLIP 리뷰"
date: 2024-10-17
---

# CLIP Review

---

**CLIP 모델(Contrastive Language-Image Pretraining)**은 **OpenAI**에서 개발한 모델로, 이미지와 텍스트를 같은 벡터 공간에서 학습해, 텍스트와 이미지 간의 연관성을 효율적으로 학습할 수 있도록 설계되었습니다. CLIP은 **대규모 이미지-텍스트 쌍**을 학습해, 주어진 텍스트 설명에 맞는 이미지를 정확히 찾거나, 주어진 이미지에 대한 텍스트 설명을 생성하는 데 탁월한 성능을 보입니다. CLIP에서의 **어텐션 메커니즘**은 텍스트와 이미지 간의 연관성을 세밀하게 학습하는 중요한 역할을 합니다.

### 1. CLIP 모델의 기본 개념

CLIP의 목표는 **이미지와 텍스트를 동일한 벡터 공간에 매핑**하여, 텍스트와 이미지가 서로 일치하는지 아닌지를 학습하는 것입니다. 이 과정에서 CLIP은 **Transformer** 아키텍처에서 사용된 **Self-Attention (자기-어텐션)** 메커니즘을 활용합니다. Self-Attention은 텍스트와 이미지 각각에 독립적으로 적용되며, 최종적으로 **이미지 임베딩**과 **텍스트 임베딩**이 동일한 공간에 위치하도록 맞추는 역할을 합니다.

CLIP은 크게 두 가지 인코더를 사용합니다:
1. **텍스트 인코더**: 텍스트를 처리하여 텍스트 임베딩을 생성합니다. 주로 **Transformer** 모델을 사용하여, 텍스트 내의 단어들 간의 관계를 학습합니다.
2. **이미지 인코더**: 이미지를 처리하여 이미지 임베딩을 생성합니다. CLIP에서는 주로 **Vision Transformer (ViT)** 또는 **ResNet**과 같은 이미지 처리 네트워크를 사용하여 이미지를 처리합니다.

이 두 인코더는 **Self-Attention 메커니즘**을 통해 각각 텍스트와 이미지의 내부 관계를 학습한 후, 두 임베딩을 결합하여 텍스트와 이미지 간의 관계를 학습합니다.

### 2. CLIP에서의 Self-Attention 메커니즘

#### Self-Attention의 기본 원리
**Self-Attention**은 각 입력 요소(단어 또는 이미지 패치)가 다른 입력 요소와의 관계를 학습하는 방식입니다. 텍스트 인코더에서는 단어들이 서로 어떻게 연관되어 있는지를 학습하고, 이미지 인코더에서는 이미지의 각 패치가 다른 패치들과 어떻게 상호작용하는지를 학습합니다.

Self-Attention 메커니즘의 주요 과정은 다음과 같습니다:

1. **Query(쿼리), Key(키), Value(값) 생성**:
   - **텍스트 인코더**에서는 입력된 단어들이 각기 **쿼리(Query), 키(Key), 값(Value)**로 변환됩니다. 각 단어는 나머지 단어들과의 관계를 학습하기 위해 쿼리와 키를 사용하여 상호작용합니다.
   - **이미지 인코더**에서는 이미지가 여러 패치로 분할되고, 각 패치가 쿼리, 키, 값으로 변환됩니다. 이미지는 작은 조각으로 나누어진 후, 각 조각이 다른 조각들과 어떻게 상호작용하는지를 학습합니다.

2. **유사도 계산**:
   - 쿼리와 키 간의 **유사도**를 계산합니다. Self-Attention 메커니즘은 쿼리와 키 사이의 내적(dot product)을 계산해, 각 쿼리가 어느 키와 가장 유사한지를 평가합니다. 이 계산은 이미지의 각 패치 또는 텍스트의 각 단어 간의 상호작용을 평가하는 중요한 단계입니다.
   
3. **가중치 적용**:
   - 유사도 계산을 통해 나온 값은 **소프트맥스(Softmax)** 함수를 통해 정규화됩니다. 이렇게 정규화된 값은 각 패치나 단어에 가중치를 부여하는 데 사용됩니다. 즉, 이미지나 텍스트의 어느 부분에 더 집중해야 할지 결정합니다.
   
4. **값(Value) 가중합**:
   - 마지막으로, 정규화된 가중치를 **값(Value)**에 적용하여 최종적인 출력 값을 계산합니다. 이를 통해 중요한 정보에 더 많은 가중치가 반영된 결과가 생성됩니다.

이 Self-Attention 메커니즘은 텍스트 인코더와 이미지 인코더 모두에서 사용됩니다. 텍스트 인코더는 각 단어가 문맥적으로 어떻게 관련되어 있는지 학습하고, 이미지 인코더는 각 패치가 이미지 내 다른 패치들과 어떻게 상호작용하는지 학습합니다.

#### Self-Attention을 통한 텍스트-이미지 매핑

Self-Attention은 텍스트와 이미지 각각에서 독립적으로 작동하지만, 최종적으로 텍스트와 이미지를 동일한 임베딩 공간에 매핑하기 위해 사용됩니다. CLIP에서 텍스트와 이미지는 각각의 인코더를 통해 학습된 후, 최종적으로 텍스트와 이미지가 **같은 공간**에 놓이게 됩니다. 이를 위해 **대조 학습(Contrastive Learning)**을 사용하여, 올바른 텍스트-이미지 쌍은 서로 가까운 벡터로, 잘못된 쌍은 서로 먼 벡터로 매핑됩니다.

### 3. CLIP에서의 어텐션 메커니즘 상세 분석

#### **텍스트 인코더에서의 어텐션**

텍스트 인코더는 **Transformer** 아키텍처를 기반으로 하며, 주어진 문장을 단어 토큰으로 변환한 후, Self-Attention 메커니즘을 통해 각 단어가 다른 단어와 어떻게 연관되는지를 학습합니다.

- **Self-Attention**은 각 단어의 중요도를 학습하여, 문장의 다른 부분에 어떻게 영향을 미치는지 평가합니다. 예를 들어, "고양이가 나무 위에 있다"라는 문장이 주어졌을 때, "고양이"라는 단어는 "나무"와 "있다"와의 관계를 학습해 더 좋은 문맥을 이해합니다.
- 이 과정에서 텍스트 인코더는 문장 내에서 단어 간의 관계를 학습하며, 최종적으로 문장의 **전체 의미를 반영한 임베딩**을 생성합니다.

#### **이미지 인코더에서의 어텐션**

CLIP의 이미지 인코더는 **Vision Transformer (ViT)**를 사용합니다. 이미지를 작은 패치들로 나눈 후, Self-Attention 메커니즘을 통해 각 패치가 다른 패치와 어떻게 상호작용하는지 학습합니다.

- **이미지 패치들 간의 관계 학습**: 이미지의 각 패치는 쿼리, 키, 값으로 변환되며, Self-Attention을 통해 각 패치가 다른 패치들과 어떻게 관련되어 있는지 학습합니다. 이로 인해 이미지 내의 특정 물체나 영역이 다른 부분과의 관계를 고려하면서 더 정확하게 분석됩니다.
- 예를 들어, 이미지에서 "고양이"와 "나무"가 있을 때, Self-Attention은 고양이 패치와 나무 패치 간의 상호작용을 학습해 이미지의 전체 구조를 이해합니다.

#### **텍스트-이미지 대조 학습(Contrastive Learning)**

CLIP의 핵심 메커니즘은 **대조 학습(Contrastive Learning)**입니다. 이를 통해 텍스트와 이미지 임베딩이 동일한 벡터 공간에 매핑됩니다.

1. **올바른 쌍의 가까움**: 텍스트와 이미지의 올바른 쌍이 서로 가까운 벡터 공간에 위치하도록 학습합니다. 예를 들어, "고양이가 나무 위에 있다"라는 텍스트와 실제 그 장면을 묘사한 이미지는 서로 가까운 임베딩 벡터를 갖습니다.
   
2. **잘못된 쌍의 멀어짐**: 반대로, 잘못된 쌍(예: "자동차가 달린다"라는 텍스트와 고양이 이미지)은 서로 먼 벡터 공간에 위치하도록 학습됩니다.

이 대조 학습을 통해 CLIP은 주어진 텍스트와 가장 잘 맞는 이미지를 찾을 수 있으며, 주어진 이미지에 대한 올바른 텍스트 설명도 생성할 수 있습니다.

### 4. CLIP에서의 어텐션 메커니즘의 장점

1. **텍스트와 이미지의 정밀한 정렬**: Self-Attention 메커니즘을 통해 텍스트의 각 단어와 이미지의 각 패치가 서로 어떻게 연결되는지 학습함으로써, 텍스트와 이미지 간의 세밀한 정렬이 가능해집니다.

2. **효율적인 학습**: 텍스트와 이미지를 병렬로 처리하면서, 중요한 부분에 더 집중하는 어텐션 메커니즘 덕분에 CLIP은 대규모 데이터셋에서도 효율적으로 학습할 수 있습니다.

3. **멀티모달 학습**

: CLIP의 어텐션 메커니즘은 텍스트와 이미지의 상호작용을 학습하여, 멀티모달 데이터(텍스트와 이미지 모두를 포함한 데이터)를 효과적으로 처리할 수 있게 합니다.

### 5. CLIP 모델의 실제 활용

- **이미지 검색**: CLIP은 주어진 텍스트에 가장 적합한 이미지를 검색하는 작업에 뛰어난 성능을 보입니다.
- **이미지 캡셔닝**: CLIP을 사용하면 이미지에 대한 정확한 설명을 생성할 수 있습니다.
- **텍스트 기반 이미지 생성**: CLIP의 텍스트-이미지 정렬 능력은 텍스트 설명을 바탕으로 이미지를 생성하는 모델에서도 활용됩니다.

---

### CLIP 모델 간단한 PyTorch 구현 예제

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
from transformers import BertModel, BertTokenizer

# 텍스트 인코더: BERT 사용
class TextEncoder(nn.Module):
    def __init__(self):
        super(TextEncoder, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.fc = nn.Linear(768, 512)  # 768은 BERT의 출력 크기, 512는 임베딩 크기

    def forward(self, input_ids, attention_mask):
        # BERT에서 텍스트 임베딩 추출
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.pooler_output  # [CLS] 토큰의 임베딩
        embedding = self.fc(cls_output)  # 차원 축소
        return embedding

# 이미지 인코더: Pretrained ResNet 사용
class ImageEncoder(nn.Module):
    def __init__(self):
        super(ImageEncoder, self).__init__()
        resnet = models.resnet50(pretrained=True)
        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # 마지막 FC 레이어 제거
        self.fc = nn.Linear(2048, 512)  # 2048은 ResNet의 출력 크기, 512는 임베딩 크기

    def forward(self, images):
        with torch.no_grad():
            features = self.resnet(images)  # 이미지 특징 추출
        features = features.view(features.size(0), -1)  # Flatten
        embedding = self.fc(features)  # 차원 축소
        return embedding

# 텍스트와 이미지 간의 유사도를 계산하는 모델
class CLIP(nn.Module):
    def __init__(self):
        super(CLIP, self).__init__()
        self.text_encoder = TextEncoder()
        self.image_encoder = ImageEncoder()

    def forward(self, input_ids, attention_mask, images):
        text_embedding = self.text_encoder(input_ids, attention_mask)
        image_embedding = self.image_encoder(images)

        # 텍스트와 이미지 임베딩의 유사도 계산 (Cosine Similarity)
        text_embedding = F.normalize(text_embedding, p=2, dim=1)
        image_embedding = F.normalize(image_embedding, p=2, dim=1)
        similarity = torch.matmul(text_embedding, image_embedding.T)  # Cosine Similarity 계산
        return similarity

# 간단한 사용 예
if __name__ == "__main__":
    # 입력 준비
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    text = ["a photo of a cat", "a photo of a dog"]
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=16)

    # 예시 이미지 (1x3x224x224 크기의 랜덤 텐서)
    images = torch.randn(2, 3, 224, 224)

    # CLIP 모델 초기화 및 Forward
    model = CLIP()
    similarity = model(inputs['input_ids'], inputs['attention_mask'], images)

    # 결과 출력
    print("Similarity between text and images:\n", similarity)
```

### 코드 설명:

1. **텍스트 인코더**:
   - **BERT**를 사용하여 입력된 텍스트를 임베딩합니다. BERT의 [CLS] 토큰 임베딩을 사용하며, 마지막에 차원 축소를 위해 선형 레이어를 추가했습니다.
   
2. **이미지 인코더**:
   - **ResNet50**을 사용하여 이미지에서 특징을 추출합니다. 마지막 분류 레이어를 제외하고, 이미지의 특징 맵을 추출한 후, 이를 임베딩 공간으로 변환합니다.
   
3. **CLIP 모델**:
   - 텍스트와 이미지의 임베딩을 계산하고, 두 임베딩의 **Cosine Similarity(코사인 유사도)**를 구합니다. 텍스트와 이미지의 유사도를 계산해, 어떤 텍스트가 어떤 이미지와 더 연관이 있는지 확인할 수 있습니다.
   
4. **간단한 사용 예**:
   - 텍스트("a photo of a cat", "a photo of a dog")와 임의의 이미지(224x224 크기의 랜덤 텐서)를 입력하여 텍스트와 이미지 간의 유사도를 계산합니다.

### 주요 포인트:
- 이 코드는 CLIP의 기본 개념을 간단하게 구현한 것으로, 텍스트와 이미지를 각각 임베딩한 후 동일한 공간에서 비교하는 과정을 포함합니다.
- **Cosine Similarity**를 사용해 텍스트와 이미지 간의 유사도를 측정하여, 텍스트 설명과 가장 잘 맞는 이미지를 찾을 수 있습니다.


CLIP 모델의 **학습 방법**은 일반적인 이미지 또는 텍스트 분류와 달리, **대조 학습(Contrastive Learning)**을 사용하여 텍스트와 이미지 간의 일치성을 학습합니다. 이 학습 방법은 매우 독특하며, 텍스트-이미지 페어(쌍)를 사용하여 서로 맞는 텍스트와 이미지가 가까운 벡터 공간에 위치하고, 맞지 않는 텍스트-이미지는 멀어지도록 학습합니다.

### CLIP의 학습 과정

1. **대규모 데이터셋 사용**:  
   CLIP은 대규모의 **이미지-텍스트 쌍**으로 학습됩니다. 예를 들어, 인터넷에서 수집한 "고양이 사진"과 "고양이에 대한 설명"과 같은 데이터 쌍을 수백만 개 이상 사용합니다. 이 데이터셋은 매우 다양한 주제와 이미지를 포함하고 있어, CLIP이 매우 일반화된 학습을 할 수 있게 만듭니다.

2. **대조 학습(Contrastive Learning)**:  
   CLIP의 가장 중요한 학습 방식은 **대조 학습(Contrastive Learning)**입니다. 이 방식은 다음과 같은 과정으로 작동합니다:
   
   - **맞는 쌍의 가깝게**: 이미지와 텍스트가 서로 일치하는 경우(예: "고양이"라는 텍스트와 실제 고양이 사진)는 그 둘이 **가까운 벡터 공간**에 위치하도록 학습됩니다.
   - **틀린 쌍의 멀어지게**: 반대로, 맞지 않는 쌍(예: "고양이" 텍스트와 자동차 사진)은 **멀어진 벡터 공간**에 위치하도록 학습됩니다.
   
   이를 통해 모델은 주어진 텍스트 설명에 맞는 이미지를 찾고, 주어진 이미지에 맞는 텍스트 설명을 할 수 있게 됩니다.

3. **Cosine Similarity(코사인 유사도)**:  
   CLIP은 이미지와 텍스트가 얼마나 잘 맞는지를 평가하기 위해 **코사인 유사도(Cosine Similarity)**를 사용합니다. 코사인 유사도는 두 벡터 간의 각도를 계산해 두 벡터가 얼마나 비슷한지 측정합니다. 두 벡터가 동일한 방향을 가리킬수록 유사도 값이 1에 가깝고, 반대 방향을 가리킬수록 -1에 가까워집니다.
   
   모델은 **이미지 임베딩**과 **텍스트 임베딩**의 코사인 유사도를 계산하여, 이를 학습 신호로 사용합니다.

### CLIP 학습의 주요 단계

1. **이미지와 텍스트를 각각 임베딩**:  
   CLIP은 두 개의 독립적인 인코더를 사용해 텍스트와 이미지를 각각 임베딩합니다.
   
   - **텍스트 인코더**: 텍스트를 입력받아 BERT 또는 Transformer와 같은 모델을 통해 임베딩을 생성합니다.
   - **이미지 인코더**: 이미지를 입력받아 ResNet 또는 Vision Transformer(ViT)와 같은 모델을 통해 임베딩을 생성합니다.

2. **임베딩 공간으로 매핑**:  
   텍스트와 이미지 인코더를 통해 생성된 임베딩은 **동일한 벡터 공간**으로 매핑됩니다. 텍스트와 이미지를 모두 같은 차원의 임베딩 벡터로 변환하여, 둘을 비교할 수 있도록 합니다.

3. **대조 손실 함수(Contrastive Loss)**:  
   CLIP에서 가장 중요한 학습 목표는 텍스트와 이미지 임베딩을 잘 맞추는 것입니다. 이를 위해 **대조 손실 함수**를 사용합니다. 이 손실 함수는 주어진 배치(batch) 내의 모든 텍스트-이미지 쌍을 비교하여, 올바른 쌍의 유사도를 최대화하고, 잘못된 쌍의 유사도를 최소화하는 역할을 합니다.
   
   - 맞는 텍스트-이미지 쌍의 코사인 유사도를 최대화합니다.
   - 틀린 텍스트-이미지 쌍의 코사인 유사도를 최소화합니다.
   
   이 과정은 **배치 내의 모든 텍스트-이미지 쌍**을 비교하기 때문에 **NCE (Noise Contrastive Estimation)** 또는 **InfoNCE** 손실로 불리기도 합니다.

#### 대조 손실 함수(Contrastive Loss)의 수식:
``` 
L = -log( exp(sim(t_i, i_i)) / Σ_j exp(sim(t_i, i_j)) )
```

- `sim(t_i, i_i)`는 텍스트-이미지 쌍의 코사인 유사도입니다.
- Σ_j는 배치 내의 모든 텍스트-이미지 쌍에 대해 유사도를 계산하는 것을 의미합니다.

이 손실 함수는 올바른 텍스트와 이미지의 유사도를 최대화하고, 잘못된 텍스트와 이미지의 유사도는 최소화하는 역할을 합니다.

### CLIP 학습 코드 간단한 예시

아래는 **PyTorch**로 간단하게 CLIP 학습 방식을 구현한 코드입니다:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# 대조 손실 함수(Contrastive Loss)
def contrastive_loss(text_embeds, image_embeds):
    # 임베딩 정규화 (normalize)
    text_embeds = F.normalize(text_embeds, p=2, dim=1)
    image_embeds = F.normalize(image_embeds, p=2, dim=1)
    
    # 텍스트와 이미지 간 유사도 계산 (Cosine Similarity)
    logits = torch.matmul(text_embeds, image_embeds.T)
    
    # 각 행에 대해 softmax 계산
    labels = torch.arange(len(text_embeds)).to(text_embeds.device)
    loss_text_to_image = F.cross_entropy(logits, labels)
    loss_image_to_text = F.cross_entropy(logits.T, labels)
    
    # 양방향 손실의 평균
    return (loss_text_to_image + loss_image_to_text) / 2

# 학습 루프
def train_clip(model, dataloader, optimizer):
    model.train()
    total_loss = 0.0
    
    for batch in dataloader:
        texts, images = batch['texts'], batch['images']
        optimizer.zero_grad()
        
        # 텍스트와 이미지 임베딩 생성
        text_embeds = model.text_encoder(texts['input_ids'], texts['attention_mask'])
        image_embeds = model.image_encoder(images)
        
        # 대조 손실 계산
        loss = contrastive_loss(text_embeds, image_embeds)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)

# 예시 모델과 학습 코드
if __name__ == "__main__":
    # 모델, 옵티마이저, 데이터 로더 설정 (예시 데이터 로더 사용)
    model = CLIP()  # CLIP 모델 (이전 코드 참고)
    optimizer = optim.Adam(model.parameters(), lr=3e-4)
    
    # 데이터 로더는 texts, images가 포함된 배치를 반환해야 함 (생략된 부분)
    for epoch in range(10):  # 10 에포크 학습
        loss = train_clip(model, dataloader, optimizer)
        print(f"Epoch {epoch + 1}, Loss: {loss}")
```

### 요약:

1. **대조 학습(Contrastive Learning)**: CLIP은 텍스트-이미지 쌍을 사용하여 서로 일치하는 쌍은 가까운 벡터로, 일치하지 않는 쌍은 먼 벡터로 학습합니다.
   
2. **Self-Attention과 Cosine Similarity**: 텍스트와 이미지 각각에서 Self-Attention을 사용해 중요한 정보를 추출하고, Cosine Similarity를 사용하여 텍스트와 이미지 간의 유사도를 측정합니다.

3. **대조 손실(Contrastive Loss)**: 대조 손실 함수는 텍스트와 이미지 간의 일치 여부를 학습하는 핵심 요소로, 주어진 배치 내에서 맞는 쌍은 유사도를 높이고, 틀린 쌍은 유사도를 낮추도록 최적화합니다.

이 방식으로 CLIP 모델은 텍스트와 이미지 간의 연관성을 학습하고, 텍스트와 일치하는 이미지를 효율적으로 찾을 수 있습니다.

## 최신 글
<ul>
  {% for post in site.posts %}
    <li>
      <a href="{{ post.url }}">{{ post.title }}</a>
      <span>{{ post.date | date: "%B %d, %Y" }}</span>
    </li>
  {% endfor %}
</ul>