---
title: "[논문리뷰] Animate-X Review"
last_modified_at: 2024-11-05
categories:
  - 논문리뷰
tags:
  - Animate-X
  - Character Animation
  - Diffusion Models
  - Motion Representation
excerpt: "다양한 캐릭터에 대해 일관된 동작 표현을 제공하는 Animate-X 모델 리뷰"
use_math: true
classes: wide
---

## ANIMATE-X: UNIVERSAL CHARACTER IMAGE ANIMATION WITH ENHANCED MOTION REPRESENTATION

Shuai Tan 1 $^{∗}$, Biao Gong 1 $^{†}$, Xiang Wang $^{2}$, Shiwei Zhang$^{2}$, Dandan Zheng $^{1}$, Ruobing Zheng$^{1}$, Kecheng Zheng$^{1}$, Jingdong Chen$^{1}$, Ming Yang1

$^{1}$Ant Group

$^{2}$Alibaba Group

{ tanshuai2001,a.biao.gong } @gmail.com, { xiaolao.wx,zhangjin.zsw } @alibaba-inc.com, { yuandan.zdd, zhengruobing.zrb,zhengkecheng.zkc,jingdongchen.cjd,m.yang } @antgroup.com

프로젝트 페이지: [https://lucaria-academy.github.io/Animate-X/](https://lucaria-academy.github.io/Animate-X/)

---

## ABSTRACT

캐릭터 이미지 애니메이션은 참조 이미지와 목표 포즈 시퀀스에서 고품질 영상을 생성하는 작업으로, 최근 몇 년간 큰 발전을 이루었습니다. 그러나 대부분의 기존 방법은 인간 형상에만 적용되어 게임 및 엔터테인먼트 산업에서 일반적으로 사용되는 의인화된 캐릭터에 대해 잘 일반화되지 않습니다. 심층 분석에 따르면, 이 제한은 운동 모델링의 불충분으로 인해 발생하며, 이는 운전 비디오의 움직임 패턴을 이해하지 못해 포즈 시퀀스를 목표 캐릭터에 강제 적용하게 됩니다. 이에 본 논문은 다양한 캐릭터 타입(X라 명명)을 위한 보편적 애니메이션 프레임워크 Animate-X를 제안합니다. 이를 위해 LDM 기반의 Animate-X는 운동 표현을 향상시키기 위해 운전 비디오에서 전반적인 움직임 패턴과 모션 간의 시간 관계를 포착하는 Pose Indicator를 도입했습니다. 이 과정에서 CLIP의 시각적 특징을 활용하여 암묵적 및 명시적 방식으로 종합적인 운동 패턴을 파악하며, 명시적 측면에서는 추론 시 발생 가능한 입력을 미리 시뮬레이션하여 LDM의 일반화를 강화합니다. 또한 의인화된 캐릭터 애니메이션 성능을 평가하기 위한 A$^{2}$Bench라는 새 벤치마크를 도입하여 Animate-X의 성능 우수성을 입증합니다. 광범위한 실험 결과, Animate-X가 최신 방법에 비해 우수한 성능을 발휘함을 확인했습니다.

## 1 INTRODUCTION

캐릭터 이미지 애니메이션 Yang et al. (2018); Zablotskaia et al. (2019b)은 참조 이미지와 목표 포즈 시퀀스에서 생생하고 고품질의 영상을 생성하는 매력적이고 도전적인 작업입니다. 최신 이미지 애니메이션 방법은 신원 보존과 운동 일관성을 균형 있게 유지하여 Hu et al. (2023); Xu et al. (2023a); Chang et al. (2023a); Jiang et al. (2022) 등에서 확인할 수 있듯이 다양한 활용 가능성을 제공합니다. GAN Goodfellow et al. (2014); Yu et al. (2023); Zhang et al. (2022b)과 생성적 확산 모델 Ho et al. (2022; 2020); Guo et al. (2023)의 혁신은 캐릭터 애니메이션 생성 성능을 변화시켰습니다. 그럼에도 불구하고 대부분의 방법은 인간 캐릭터에만 적용됩니다. 실질적으로 "캐릭터"라는 개념은 인간에 국한되지 않으며, 게임이나 애니메이션에서 널리 사용되는 의인화된 캐릭터 등 X로 불리는 보다 폭넓은 개념을 포함합니다. 이러한 도메인으로 모델을 확장하기 어려운 이유는 (1) 인간 중심의 데이터셋이 주를 이루고 있으며, (2) 현재의 운동 표현의 일반화 능력이 제한적이기 때문입니다.

비인간 캐릭터의 한계는 Fig. 5에서 명확히 드러납니다. 확산 모델은 주어진 포즈를 복제하려 할 때, 인간 특유의 특징을 포함하여 캐릭터에 비정상적인 왜곡을 유발합니다. 즉, 모델은 신원 보존과 운동 일관성을 상충되는 목표로 간주하고, 이를 균형 잡기가 어려워 결과적으로 운동 제어가 우위를 점하게 됩니다. 이러한 문제는 의인화된 캐릭터에 특히 두드러지며, 이들의 신체 구조가 인간 해부학과 다른 특징(예: 과도하게 큰 머리나 팔의 부재 등)을 가질 수 있기 때문입니다. 주요 원인은 포즈 조건에서 추출한 운동 표현이 독특한 신체적 특징을 지닌 일반 만화 캐릭터에 일반화하기 어렵다는 데 있습니다. 이는 엄격한 포즈 일관성을 유지하는 대가로 신원 보존을 과도하게 희생하게 되어 이 목표들 간의 불합리한 거래를 초래합니다.

이 문제를 해결하기 위해서는 현재의 포즈 조건을 유지하면서도 운동 표현의 유연성을 향상시키는 자연스러운 접근법이 필요합니다. 이를 통해 지나치게 정밀한 포즈와 낮은 신원 충실성 간의 불합리한 거래를 방지할 수 있습니다. 이를 위해 기존 방법의 두 가지 주요 한계를 식별했습니다. 첫째, 희소한 키포인트를 연결하여 구성된 단순한 2D 포즈 골격은 이미지 수준의 세부 정보가 부족하여 참조 비디오의 본질을 캡처하지 못합니다. 둘째, 자체 구동 재구성 전략은 몸의 형태에 의해 참조와 포즈 골격을 정렬하여 애니메이션을 단순화하지만, 추론 시 형태 차이를 간과합니다. 이러한 한계점을 바탕으로 암묵적 및 명시적 관점에서 새로운 Pose Indicator를 설계했습니다.

본 논문에서는 캐릭터 X의 애니메이션을 위한 Animate-X를 제안합니다. 생성적 확산 모델 Rombach et al. (2022)에서 영감을 받아, 3D-UNet Blattmann et al. (2023)을 노이즈 제거 네트워크로 사용하고 운동 특징과 피규어 신원을 조건으로 제공합니다. 운전 비디오의 핵심 운동을 완전히 포착하기 위해, Implicit Pose Indicator (IPI)와 Explicit Pose Indicator (EPI)로 구성된 Pose Indicator를 도입했습니다. IPI는 CLIP 이미지 특징을 이용하여 운동 패턴과 관계를 분리하여 포즈 골격으로 직접 표현할 수 없는 필수적인 운동 특징을 추출합니다. 한편, EPI는 학습 중에 참조 이미지와 포즈의 불일치를 시뮬레이션하여 명시적 포즈 특징을 생성하는 능력을 강화합니다. 암묵적 및 명시적 특징을 결합한 Animate-X는 캐릭터 일반화와 포즈 강건성을 보여주며, 인간 데이터셋만으로 학습된 상태에서도 보편적인 X 캐릭터 애니메이션을 가능하게 합니다. 또한, 500개의 의인화된 캐릭터와 대응하는 댄스 비디오를 포함한 새로운 Animated Anthropomorphic Benchmark (A$^{2}$Bench)를 도입하여 다른 종류의 캐릭터에 대한 Animate-X의 성능을 평가합니다. 주요 기여는 다음과 같이 요약됩니다.

- · 이미지 조건 포즈 안내 비디오 생성의 보편성과 특히 매력적인 의인화된 캐릭터에 대한 일반화를 가능하게 하는 Animate-X를 제시합니다. 엄격한 포즈 정렬이 필요 없는 일반 만화 이미지 애니메이션을 위한 첫 작업입니다.

- · 운동에 대한 재고가 Pose Indicator를 제안하게 했으며, 이는 의인화된 캐릭터에 적합한 운동 표현을 암묵적 및 명시적 방식으로 추출하여 Animate-X의 강건성을 향상시킵니다.

- · 인기 있는 데이터셋이 제한된 캐릭터 다양성을 지닌 인간 비디오만 포함하고 있기 때문에, 의인화된 캐릭터에 대한 성능을 평가하기 위해 A$^{2}$Bench를 제시했습니다. 광범위한 실험 결과 Animate-X가 A$^{2}$Bench와 현재 인간 애니메이션 벤치마크에서 정량적 및 정성적으로 경쟁 방법을 능가함을 보여줍니다.

## 2 RELATED WORK

### 2.1 DIFFUSION MODELS FOR IMAGE/VIDEO GENERATION

최근 몇 년간, diffusion 모델들은 Song et al. (2021); Ho et al. (2020) 강력한 생성 능력을 보여주었으며, 이미지 생성 기술을 Nichol et al. (2022); Ramesh et al. (2022); Mou et al. (2023); Huang et al. (2023); Zhang et al. (2023a); Liu et al. (2023) 등에서 일상적인 생산성 도구로 발전시켰습니다. 특히 DALL-E 2 Ramesh et al. (2022)와 Imagen Saharia et al. (2022)과 같은 선구적인 작업들이 고품질 이미지 생성에서 diffusion 모델의 놀라운 잠재력을 보여주었습니다. Stable Diffusion Rombach et al. (2022)과 같은 모델은 확장성과 효율성을 잘 균형있게 맞추어 diffusion 기반 이미지 생성이 다양한 응용 분야에 접근 가능하고 유연하게 활용될 수 있도록 했습니다. 비디오 생성 분야에서도 diffusion 모델은 Singer et al. (2023); Wang et al. (2023a; 2024c); Wu et al. (2023); Chai et al. (2023); Ceylan et al. (2023); Guo et al. (2023); Zhou et al. (2022); An et al. (2023); Xing et al. (2023); Qing et al. (2023); Yuan et al. (2023); Tan et al. (2024d); Gong et al. (2024) 등의 연구를 통해 획기적인 진전을 이루고 있습니다. 이러한 방법들은 시공간 모델링을 결합하여 현실적인 운동 역학을 생성하고 시간적 일관성을 보장하며, 비디오 콘텐츠 생성의 생성 모델에서 중요한 진보를 이뤄냈습니다. 본 연구에서는 이미지 애니메이션 작업을 위한 캐릭터 중심의 조건부 비디오 생성에 집중하며, 정적 이미지를 원하는 동작에 따라 역동적인 애니메이션으로 변환하는 접근 방식을 채택하였습니다. 이는 이미지와 비디오 생성 간의 격차를 줄이며, diffusion 모델의 유연성과 적응성을 부각시켜 흥미로운 시각적 내러티브를 창출할 수 있음을 보여줍니다.

### 2.2 POSE-GUIDED CHARACTER MOTION TRANSFER

캐릭터 이미지 애니메이션은 소스 캐릭터의 움직임을 대상 캐릭터에 전송하는 작업을 목표로 하며 Zhang et al. (2024); Chang et al. (2023b), 애니메이션 품질과 다양성을 향상시키기 위해 많은 발전을 이뤄왔습니다. 초기 작업들 Li et al. (2019); Siarohin et al. (2019b; 2021b); Zhao & Zhang (2022b); Tan et al. (2024a); Wang et al. (2022); Tan et al. (2024c;b; 2023)은 주로 Generative Adversarial Networks (GANs)를 활용하여 애니메이션화된 인간 이미지를 생성했습니다. 하지만 이러한 GAN 기반 모델은 생성된 결과물에 다양한 인공물이 나타나는 문제를 자주 겪습니다. diffusion 모델의 등장으로 Shen et al. (2024); Zhu et al. (2024) 연구자들은 GAN을 넘어서기 위해 diffusion 모델을 탐구했습니다. 예를 들어 Disco Wang et al. (2023b)는 ControlNet Zhang et al. (2023b)를 활용하여 인간의 댄스를 생성하는 잠재력을 보여주었으며, 이를 통해 diffusion 모델이 동적 인간 포즈를 생성하는 데에 큰 가능성을 보여주었습니다. 이후 MagicAnimate Xu et al. (2023b)와 Animate Anyone Hu et al. (2023)은 Transformer 기반의 temporal attention 모듈 Vaswani (2017)을 도입하여 애니메이션의 시간적 일관성을 강화하고 더 부드러운 움직임 전환을 실현했습니다. Mamba Gu & Dao (2023); Gu et al. (2021)의 효율적인 시간 모델링 개념에서 영감을 받아, Unianimate Wang et al. (2024b)은 긴 시퀀스 처리에서 Mamba를 사용하여 효율적인 시간적 모델링을 구현했습니다.

이러한 접근 방식들이 애니메이션의 현실성을 향상시켰지만, 여전히 주된 한계는 참조 이미지와 운전 비디오 간의 엄격한 정렬이 필요하다는 점입니다. 이는 포즈를 쉽게 추출할 수 없는 의인화된 캐릭터와 같은 시나리오에서는 제한적이며, 기괴하고 만족스럽지 않은 결과를 초래하는 경우가 많습니다. 이에 반해, 본 연구에서는 강력하고 유연한 운동 표현을 채택하여 포즈 정렬에 대한 의존도를 줄였습니다. 이를 통해 이전의 방법들이 비정렬 포즈에서 어려움을 겪던 경우에도 고품질 애니메이션을 생성할 수 있습니다. 본 방식은 다양한 컨텍스트(X 캐릭터)에서 캐릭터 이미지 애니메이션의 유연성과 적용 가능성을 향상시킵니다.

## 3 METHOD

본 연구에서는 참조 이미지 \(I^r\)와 운전 비디오 \(I_d^{1:F}\)와 일관된 신원을 유지하면서 몸 동작을 유지하는 애니메이션 비디오를 생성하고자 합니다. 이전 작업과는 달리, 우리의 주요 목표는 엔터테인먼트 산업에서 널리 사용될 수 있는 인간 이외의 다양한 캐릭터(특히 의인화된 캐릭터) 애니메이션을 생성하는 것입니다.

### 3.1 PRELIMINARIES OF LATENT DIFFUSION MODEL

Diffusion 모델(DM)은 노이즈를 통해 데이터 생성 과정을 모델링하는 확률적 프로세스를 학습합니다. 고차원 RGB 공간에서의 전통적인 픽셀 기반 diffusion 모델의 높은 계산 요구를 완화하기 위해, Latent Diffusion Model (LDM) Rombach et al. (2022)은 사전 학습된 변분 오토인코더(VAE) Kingma (2013)를 사용하여 프로세스를 저차원의 잠재 공간으로 이동시킵니다. 이는 입력 데이터를 압축된 잠재 표현 \(z_0\)로 인코딩하며, 여러 단계에 걸쳐 가우시안 노이즈를 점진적으로 추가하여 생성 능력을 유지하면서도 계산 요구를 줄입니다. 이 과정은 다음과 같이 형식화할 수 있습니다.

\[ q(z_t | z_{t-1}) = N(z_t ; \sqrt{1 - \beta_t} z_{t-1}, \beta_t I) , \]

여기서 \(\beta_t \in (0, 1)\)은 노이즈 일정표를 나타냅니다. \(t \in 1, 2, \dots, T\)가 증가함에 따라 원래 \(z_0\)에 적용되는 누적 노이즈가 증가하여 \(z_t\)가 점진적으로 무작위 가우시안 노이즈와 유사하게 됩니다.

순방향 diffusion 과정과 비교하여, 역방향 노이즈 제거 과정 \(p_\theta\)는 노이즈가 포함된 입력 \(z_t\)에서 깨끗한 샘플 \(z_0\)을 재구성하는 것을 목표로 합니다. 우리는 노이즈 제거 단계를 다음과 같이 나타냅니다.

\[ p_\theta(z_{t-1} | z_t) = N(z_{t-1} ; \mu_\theta(z_t, t), \Sigma_\theta(z_t, t)) , \]

여기서 \(\mu_\theta(z_t, t)\)는 역 diffusion 과정의 목표를 추정한 것이며, 일반적으로 파라미터 \(\theta\)를 가진 diffusion 모델 \(\epsilon_\theta\)를 통해 이루어집니다. 시간 차원을 모델링하기 위해, 비디오 생성 방법 Hu et al. (2023); Wang et al. (2023c)에서 노이즈 제거 모델 \(\epsilon_\theta\)는 일반적으로 3D-UNet 구조 Blattmann et al. (2023) 기반으로 구축됩니다. 주어진 입력 조건 가이던스 \(c\)에 대해, 예측된 노이즈와 실제 노이즈 간의 차이를 줄이기 위해 보통 L2 손실을 사용합니다.

\[ L = \mathbb{E}_\theta \left[ \| \epsilon - \epsilon_\theta(z_t, t, c) \|^2 \right] . \]

역방향 노이즈 제거 단계가 완료되면, 예측된 깨끗한 잠재 표현이 VAE 디코더를 거쳐 픽셀 공간에서 예측된 비디오로 재구성됩니다.

### 3.2 POSE INDICATOR

운동 표현을 추출하기 위해, 기존 연구들은 주로 DWPose Yang et al. (2023)를 사용하여 운전 비디오 \(I_d^{1:F}\)에서 포즈 키포인트를 감지하고 이를 포즈 이미지 \(I^p\)로 시각화한 후, 자체 구동 재구성 전략을 통해 학습합니다. 그러나 이는 1절에서 언급된 몇 가지 제한을 초래합니다. (1) 단일 포즈 골격은 이미지 수준의 세부 정보가 부족하여 참조 비디오의 본질, 즉 운동에 의한 변형이나 전반적인 운동 패턴을 캡처할 수 없습니다. (2) 자체 구동 재구성 학습 전략은 신체 형태 면에서 참조 이미지와 포즈 이미지를 자연스럽게 정렬시키므로, 추론 중에 참조 이미지와 포즈 이미지 간의 잠재적인 신체 형태 차이를 간과하게 됩니다. 이러한 문제를 해결하기 위해, 우리는 암묵적 Pose Indicator (IPI)와 명시적 Pose Indicator (EPI)로 구성된 Pose Indicator를 제안합니다.

#### Implicit Pose Indicator (IPI)

첫 번째 제한점을 해결하기 위해, CLIP 이미지 특징 \(f_d^\phi = \Phi(I_d^{1:F})\)을 이용하여 운전 비디오에서 통합된 운동 표현을 추출합니다. CLIP은 contrastive 학습을 통해 관련된 이미지와 텍스트의 임베딩을 정렬하며, 여기에는 외형, 움직임, 공간 관계 등을 포함한 설명이 포함될 수 있습니다. 따라서 CLIP 이미지 특징은 운동 패턴 및 관계를 포함하는 고도로 얽혀 있는 표현으로, 애니메이션 생성에 유용한 특징을 담고 있습니다.

IPI는 N개의 cross-attention과 feed-forward network (FFN) 레이어로 구성된 경량 추출기 \(P\)를 도입하여 CLIP 이미지 특징을 키(K)와 값(V)로 사용합니다. 키포인트 \(p_d\)는 운동에 대한 직접적인 설명을 제공하므로 이를 쿼리(Q)로 사용하기 위해 transformer 기반 인코더를 설계하였으며, 쿼리 벡터 \(q_p\)를 통해 운동을 추출합니다. 하지만 단일 키포인트만을 이용한 운동 모델링은 단순하여 근본적인 운동 패턴을 잃게 됩니다. 이를 보완하기 위해, 쿼리 transformer 아키텍처에서 영감을 받아 학습 가능한 쿼리 벡터 \(q_l\)를 도입하여 키포인트를 보완하였으며, 병합된 쿼리 \(q_m = q_p + q_l\)와 \(f_d^\phi\)를 입력으로 사용하여 암묵적 포즈 인디케이터 \(f_i\)를 생성하여 단순한 2D 포즈 골격으로는 표현할 수 없는 운동의 본질적인 표현을 포함하도록 하였습니다.

#### Explicit Pose Indicator (EPI)

두 번째 제한점을 해결하기 위해, EPI는 모델이 추론 중에 불일치한 입력 쌍을 처리하도록 학습하도록 설계되었습니다. 핵심은 참조 이미지와 포즈 이미지 사이의 불일치를 시뮬레이션하면서 주어진 운전 비디오 \(I_d^{1:F}\)와 운동이 일관성을 유지하도록 하는 것입니다. 이 과정에서 Pose Realignment와 Pose Rescale의 두 가지 포즈 변환 스킴을 도입했습니다.

Pose Realignment 방식에서는, 먼저 훈련 세트에서 포즈 이미지를 포함하는 포즈 풀을 구축합니다. 각 훈련 단계에서 참조 이미지 \(I^r\)와 운전 포즈 \(I^p\)를 샘플링하고, 포즈 풀에서 정렬 기준 포즈 \(I^{p}_{\text{anchor}}\)를 무작위로 선택합니다. 이 앵커 포즈는 운전 포즈를 정렬하는 기준이 되며, 이 정렬된 포즈를 \(I^{p}_{\text{realign}}\)으로 생성합니다. 그러나 우리가 애니메이션하고자 하는 캐릭터는 의인화된 캐릭터인 경우가 많아 인간과는 형태적으로 크게 다를 수 있습니다. 예를 들어, 머리와 어깨 비율이 다르거나 다리가 짧거나, 팔이 없는 경우도 있습니다(Fig. 1 및 Fig. 5 참조). 따라서 포즈 정렬만으로 이러한 변형을 충분히 포착할 수 없습니다. 

이 문제를 해결하기 위해 우리는 추가로 Pose Rescale을 도입했습니다. 구체적으로, 신체 길이, 다리, 팔, 목, 어깨의 길이 조정, 얼굴 크기 변경, 특정 신체 부위 추가 또는 제거 등을 포함한 키포인트 재조정 작업을 정의하여 이러한 변환을 rescale 풀에 저장했습니다. 정렬된 포즈 \(I^{p}_{\text{realign}}\)을 얻은 후, 이 풀에서 특정 확률에 따라 변환을 무작위로 선택해 적용하여 최종 변환된 포즈 \(I^{p}_n\)을 생성합니다(Appendix A에 추가 변환 예시 제공). 여기서 포즈 변환이 적용될 확률을 \(\lambda \in [0, 1]\)로 설정하고, 1 - \(\lambda\) 확률로 포즈 이미지는 변경되지 않은 상태로 유지됩니다. 이후 \(I^{p}_n\)은 Pose Encoder를 통해 명시적 특징 \(f_e\)로 인코딩됩니다.

### 3.3 FRAMEWORK AND IMPLEMENT DETAILS

이전 연구의 성공을 바탕으로 Hu et al. (2023); Zhang et al. (2024), Animate-X는 여러 인코더로 구성된 주요 프레임워크를 따르며 비디오 생성을 위한 3D-UNet Wang et al. (2023a;c); Blattmann et al. (2023)을 사용합니다. Fig. 2에 나타난 것처럼, 주어진 참조 이미지 \(I^r\)에 대해 사전 학습된 CLIP Image Encoder \(\Phi\) Radford et al. (2021)을 사용하여 \(I^r\)에서 외형 특징 \(f_r^\phi\)를 추출합니다. 프레임워크의 파라미터를 줄이고 외형 정렬을 촉진하기 위해, 이전 연구에서 제시된 Reference Net을 배제하고 대신 VAE 인코더 \(E\)를 사용하여 \(I^r\)에서 잠재 표현 \(f_r^e\)을 추출하여 denoising network \(\epsilon_\theta\)의 입력으로 직접 사용합니다 Wang et al. (2024b). 

운전 비디오 \(I_d^{1:F}\)에 대해, 우리는 DWPose Yang et al. (2023)와 CLIP Image Encoder \(\Phi\)를 통해 포즈 키포인트 \(p_d\)와 CLIP 특징 \(I_d\)를 감지합니다. 이어서 IPI와 EPI (3.2절에서 소개됨)는 각각 암묵적 잠재 표현 \(f_i\)와 명시적 잠재 표현 \(f_e\)를 추출합니다. 명시적 표현 \(f_e\)는 먼저 노이즈가 포함된 잠재 \(ϵ\)와 채널 차원에서 결합된 다음, 시간 차원에서 \(f_r^e\)와 결합하여 결합된 특징 \(f_{\text{merge}}\)를 형성합니다. 이후 결합된 특징은 비디오 diffusion 모델 \(\epsilon_\theta\)에 입력되어 외형 정렬과 운동 모델링을 동시에 수행합니다. diffusion 모델 \(\epsilon_\theta\)는 Spatial Attention, Motion Attention, Temporal Attention으로 구성된 여러 계층을 포함하고 있습니다. Spatial Attention은 \(f_{\text{merge}}\)와 \(f_r\)에서 입력을 받고 교차 주의 메커니즘(CA)을 통해 \(I_r\)의 신원 조건과 \(I_d\)의 운동 조건을 융합하여 중간 표현 \(x\)를 생성합니다. 운동 일관성을 더욱 향상시키기 위해, 암묵적 표현 \(f_i\)는 Motion Attention 모듈로 제공되며, 이는 잔차 연결을 통해 \(x + CA(x, f_i)\) 형태의 표현 \(x'\)을 결과로 만듭니다. 긴 시퀀스 처리를 위한 Mamba의 효율성을 고려하여 Gu & Dao (2023), Temporal Attention 모듈로 활용하여 시간적 일관성을 유지합니다.

#### Training and Inference

모델의 포즈와 참조 이미지 불일치에 대한 강건성을 개선하기 위해, 두 가지 주요 학습 전략을 채택했습니다. 첫째, EPI에서 높은 변환 확률 \(\lambda\) (98% 이상)을 설정하여 모델이 다양한 불일치 시나리오를 처리할 수 있도록 했습니다. 둘째, 입력 조건에 대해 일정 비율로 랜덤 드롭아웃을 적용 Wang et al. (2024b). 이와 같이, 학습 중 참조 이미지와 운전 비디오는 동일한 인간 댄스 비디오에서 가져오지만, 추론 단계(Fig. 9(b))에서는 다른 외형의 참조 이미지와 운전 비디오를 처리할 수 있습니다.

### 3.4 A $^{2}$BENCH

Animate-X의 주요 작업은 의인화된 캐릭터를 생생하고 부드러운 동작으로 애니메이션하는 것입니다. 그러나 현재 공개된 데이터셋 Jafarian & Park (2021); Zablotskaia et al. (2019a)는 주로 인간 애니메이션에 집중되어 있어, 다양한 의인화된 캐릭터와 해당 댄스 비디오를 포괄하기에 부족합니다. 이러한 격차로 인해 의인화된 캐릭터 애니메이션에서 다른 방법들을 정량적으로 평가하는 데에 적합하지 않습니다.

이를 해결하기 위해, 우리는 다양한 방법의 성능을 포괄적으로 평가할 수 있는 Animated Anthropomorphic character Benchmark (A$^{2}$Bench)를 제안합니다. 구체적으로, GPT-4 OpenAI (2024)를 통해 프롬프트 템플릿을 생성하여 각 프롬프트에 의인화된 캐릭터의 텍스트 설명이 포함되도록 했습니다(Appendix B.2 참조). KLing AI Technology (2024)의 강력한 이미지 생성 능력에서 영감을 받아 생성된 프롬프트를 Text-To-Image 모듈에 입력하여 프롬프트에 맞는 의인화된 캐릭터 이미지를 생성했습니다. 이후 Image-To-Video 모듈을 사용하여 이미지의 캐릭터들이 생동감 있게 춤을 추도록 만들었습니다. 각 프롬프트에 대해 4번 반복하고 가장 만족스러운 이미지-비디오 쌍을 선택하여 이 프롬프트에 해당하는 최종 출력으로 삼았습니다. 이와 같이 500개의 의인화된 캐릭터와 대응하는 댄스 비디오를 수집했습니다(Fig. 3 참조). Appendix B에 자세한 내용을 참고해 주세요.

## 4 EXPERIMENTS

### 4.1 EXPERIMENTAL SETTINGS

**Dataset**. 우리는 약 9,000개의 인간 비디오를 인터넷에서 수집하고, 추가적으로 TikTok 데이터셋 Jafarian & Park (2021)과 Fashion 데이터셋 Zablotskaia et al. (2019a)를 사용했습니다. 이전 연구 Hu et al. (2023); Zablotskaia et al. (2019a); Jafarian & Park (2021)를 따르며, TikTok과 Fashion 데이터셋에서 각각 10개와 100개의 비디오를 정성적 및 정량적 비교에 사용했습니다. 추가적으로, 3.4절에서 새롭게 제안된 A$^{2}$Bench에서 선택된 100개의 이미지-비디오 쌍에 대해 실험을 수행했습니다. 공정한 비교를 위해 A$^{2}$Bench의 데이터는 학습 세트에 포함되지 않았으며, 오직 정량적 결과를 평가하고 참조 이미지 사례를 제공하는 데에만 사용되었습니다.

**Evaluation Metrics**. 생성된 결과의 시각적 품질을 측정하기 위해 Appendix B.1에 설명된 PSNR Hore & Ziou (2010), SSIM Wang et al. (2004), L1, LPIPS Zhang et al. (2018)와 같은 널리 사용되는 이미지 메트릭을 사용했습니다. 추가적으로, 생성된 비디오 분포와 실제 비디오 분포 간의 차이를 정량화하기 위해 FID Heusel et al. (2017), FID-VID Balaji et al. (2019), FVD Unterthiner et al. (2018)을 도입했습니다.

### 4.2 EXPERIMENTAL RESULTS

**Quantitative Results**. Animate-X는 의인화된 캐릭터의 애니메이션화에 중점을 두고 있으므로, 포즈 골격을 정확하게 추출할 수 없는 경우가 많습니다(Fig. 5 참조). 이 경우 정량적 결과를 계산하기 위해 새로운 비교 설정을 마련했습니다. A$^{2}$Bench의 각 사례(즉, 참조 이미지 \(I_a\)와 포즈 \(P^a\), Fig. 4 참조)에 대해 무작위로 한 인간의 포즈 이미지 \(P^b\)를 선택하고, 의인화된 캐릭터의 포즈 \(P_a\)를 이를 기준으로 정렬하여, 포즈 \(p_a^b\)가 \(P_a\)의 움직임을 유지하면서 \(p^b\)와 동일한 신체 형태(예: 비만/마름, 키가 큼/작음 등)를 가지도록 했습니다. 최종적으로, 의인화된 캐릭터 \(I_a\)와 정렬된 운전 포즈 이미지 \(p_a^b\)를 모델에 입력하여 생성된 결과와 원본 의인화된 캐릭터 댄스 비디오를 A$^{2}$Bench에서 정량적 메트릭을 계산하는 데 사용했습니다. 

이 설정에서 우리는 Animate Anyone Hu et al. (2023), Unianimate Wang et al. (2024b), MimicMotion Zhang et al. (2024), ControlNeXt Peng et al. (2024), 그리고 MusePose Tong et al. (2024)와 같은 포즈 이미지(예: Fig. 4의 \(P^b\))를 입력으로 사용하는 방법들과 비교했습니다. Tab. 1의 정량적 결과에서 볼 수 있듯이, Animate-X는 모든 메트릭에서 다른 방법들에 비해 현저히 우수한 성능을 보였습니다. 공정한 비교를 위해, 우리는 A$^{2}$Bench 데이터를 학습 데이터로 사용하지 않아 과적합을 피했으며, 다른 비교 방법들과 동일한 조건을 유지했습니다.

이전 연구에서 정량적 결과를 자가 구동(self-driven) 및 재구성(reconstruction) 방식으로 평가한 것처럼, 우리는 추가적으로 (a) GAN 기반 이미지 애니메이션 작업: FOMM Siarohin et al. (2019a), MRAA Siarohin et al. (2021a), LIA Wang et al. (2022), (b) Diffusion 모델 기반 이미지 애니메이션 작업: DreamPose Karras et al. (2023), MagicAnimate Xu et al. (2023a)과 비교하여 Tab. 2에 결과를 제시했습니다. 결과는 우리 방법이 모든 메트릭에서 최고 성능을 달성했음을 보여줍니다. 또한, TikTok과 Fashion의 인간 데이터셋에 대한 정량적 결과는 Tab. 7과 Tab. 8에 제공되었으며, Appendix D.2에서 자세한 내용을 확인할 수 있습니다. Animate-X는 Unianimate와 비교할 만한 점수를 기록했으며, 다른 SOTA 방법을 능가하는 성능을 보여 인간과 의인화된 캐릭터 벤치마크에서 모두 우수성을 입증했습니다.

**Qualitative Results**. 의인화된 캐릭터 애니메이션의 정성적 비교는 Fig. 5에 제시되어 있습니다. GAN 기반 LIA Wang et al. (2022)은 데이터셋에 특화되어 잘 일반화되지 못하는 경향이 있으며, Siarohin et al. (2019b)와 같이 특정 데이터셋에서만 작동할 수 있음을 확인했습니다. 강력한 생성 능력을 갖춘 diffusion 모델을 사용한 Animate Anyone Hu et al. (2023)은 더 높은 해상도의 이미지를 생성하지만, 이미지의 신원이 변화하며 정확한 참조 포즈 동작을 생성하지 못합니다. MusePose Tong et al. (2024), Unianimate Wang et al. (2024b), MimicMotion Zhang et al. (2024)은 운동 전달 정확도를 개선하였으나, 이 방법들은 참조 캐릭터가 아닌 새로운 사람을 생성하는 문제가 있습니다. 

ControlNeXt는 위의 두 가지 방법의 장점을 결합하여 신원과 운동 전달 일관성을 어느 정도 유지했으나, 일부 결과는 부자연스러운 결과를 초래하였으며 예를 들어, Fig. 5에서 토끼의 귀나 바나나의 다리 부분에서 이러한 부작용이 관찰되었습니다. 반면 Animate-X는 참조 이미지와의 일관성을 유지하면서 더욱 표현적이고 과장된 캐릭터 동작을 생성하여, 타겟 캐릭터의 정적인 동작을 단순히 복사하는 것 이상의 결과를 보여주었습니다. 또한 Fig. 6에 긴 비디오 생성에 대한 비교를 제시하였으며, Unianimate는 참조 없이 새로운 여성을 생성하여 주어진 포즈 이미지에 따라 춤을 추게 한 반면, Animate-X는 참조 이미지를 귀엽게 애니메이션화하면서도 외형과 시간적 연속성을 유지하여 원래 존재하지 않는 부분을 생성하지 않았습니다. 요약하면, Animate-X는 외형 유지와 정확하고 생동감 있는 애니메이션을 고도로 시간적 일관성을 가지고 생성할 수 있음을 보여줍니다. 자세한 내용은 Appendix D.1에서 확인하십시오.

**User Study**. 우리 방법과 SOTA 방법들의 품질을 인간의 관점에서 평가하기 위해, 10명의 참가자와 블라인드 사용자 연구를 수행했습니다. 구체적으로, 우리는 A$^{2}$Bench에서 10개의 캐릭터를 무작위로 선택하고 웹사이트에서 10개의 운전 비디오를 수집했습니다. 테스트된 6개 방법 각각에 대해 10개의 애니메이션 클립을 생성하여 총 60개의 클립을 얻었습니다. 각 참가자에게 동일한 입력 세트로 생성된 다른 방법의 두 결과를 제시하고, 시각적 품질, 신원 보존, 시간적 일관성 측면에서 어느 결과가 더 나은지 선택하도록 요청했습니다. 이 과정은 C6 2회 반복되었습니다. Tab. 3에 요약된 결과에서, 우리 방법이 모든 측면에서 다른 방법들을 크게 능가함을 확인할 수 있었으며, 이는 그 우수성과 효율성을 입증합니다. 자세한 내용은 Appendix C에 나와 있습니다.

### 4.3 ABLATION STUDY

**Ablation on Implicit Pose Indicator**. Implicit Pose Indicator의 기여도를 분석하기 위해, Animate-X에서 IPI를 제거한 w/o IPI 버전을 사용하여 Baseline 및 Animate-X와 비교했습니다. Fig. 7의 첫 번째 행에서 보듯이, Baseline은 참조 이미지와 상당히 다른 외형을 가진 사람을 생성합니다. EPI의 도움으로 이 문제가 부분적으로 해결되었지만, IPI가 없는 경우 Ours와 비교하여 여전히 이상한 요소가 나타나고, 인간 같은 손이 생기는 문제가 발생합니다(파란색 원으로 표시됨). 

IPI 구조에 대한 자세한 분석을 위해 몇 가지 변형을 설정했습니다. (1) IPI 제거: w/o IPI, (2) 학습 가능한 쿼리 제거: w/o LQ, (3) DWPose 쿼리 제거: w/o DQ. Tab. 4에 제시된 정량적 결과에서 볼 수 있듯이, 전체 IPI를 제거하면 가장 성능이 낮았으며, IPI 모듈

을 수정하여 일부 개선이 있었지만 최종 Animate-X 결과보다 여전히 부족함을 확인했습니다. 이는 현재의 IPI 구조가 가장 합리적이며 최고의 성능을 달성함을 시사합니다.

**Ablation on Explicit Pose Indicator**. 두 번째 행의 Fig. 7에 EPI 설정의 가시적 결과를 제시하여 EPI를 제거한 경우를 보여주었습니다. EPI가 없으면 IPI 덕분에 팬더의 외형은 유지되지만, 모델은 팬더의 귀를 팔로 오인하고 다리를 늘여 포즈 이미지와 일치시키려 합니다(빨간색 원으로 표시됨). 반면, EPI의 도움으로 이러한 문제들이 완전히 해결되었습니다. 다양한 포즈 변환 쌍에 대한 더 자세한 ablation 실험을 수행했습니다. (1) 전체 EPI 제거: w/o EPI, (2) Pose Realignment 제거: w/o Realignment, (3) Pose Rescale 제거: w/o Rescale. Tab. 4의 결과에서 Pose Realignment가 가장 큰 기여를 한다는 것을 발견했습니다. 이는 추론 중 불일치 사례를 시뮬레이션하는 것이 중요한 요소임을 나타냅니다.

요약하면 다음과 같은 결론을 도출할 수 있습니다. (1) IPI는 외형 보존을 촉진하며 참조 이미지에 존재하지 않는 콘텐츠가 생성되는 것을 방지합니다(예: 인간 팔). (2) EPI는 참조 이미지와 자연스럽게 정렬되지 않은 포즈 이미지의 강제 정렬을 방지하여 팬더의 귀와 같이 정적이어야 할 부분이 의도치 않게 애니메이션화되는 것을 피할 수 있습니다(Fig. 7 참조). 자세한 내용은 Appendix D.4를 참조하십시오.

## 5 CONCLUSIONS

이 연구에서는 다양한 캐릭터(X로 명명)의 애니메이션을 일반화할 수 있는 새로운 캐릭터 애니메이션 접근 방식인 Animate-X를 제안했습니다. 신원 보존과 운동 일관성 간의 불균형 문제를 해결하기 위해, 암묵적 및 명시적 특징을 활용하여 모델의 운동 이해를 향상시키는 Pose Indicator를 도입했습니다. 이를 통해 Animate-X는 강력한 일반화와 강건성을 보여주며, 다양한 X 캐릭터 애니메이션을 가능하게 합니다. 제안된 프레임워크는 신원 보존과 운동 일관성 면에서 최신 방법들보다 우수한 성능을 보이며, 공개 데이터셋과 새롭게 소개된 A$^{2}$Bench (의인화된 캐릭터를 포함한 벤치마크)에서 실험적으로 입증되었습니다. 제한 사항과 윤리적 고려 사항은 Appendix E에 나와 있습니다.