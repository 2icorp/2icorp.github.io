---
title: "[논문리뷰] OmniGen"
last_modified_at: 2024-10-17
categories:
  - 논문리뷰
tags:
  - Sana
  - Text-to-Image
  - Diffusion Models
  - Autoencoder
  - High-Resolution
excerpt: ""
use_math: true
classes: wide
---

# Sana Review

---

제목: Sana: Efficient High-Resolution Text-to-Image Generation with Deep Compression and Linear Attention
저자: Sanjay Subramanian, Chitwan Saharia, William Chan, Mohammad Norouzi, David Fleet, Peyman Milanfar
Sana GitHub Repository
Sana arXiv Paper

**[Sana GitHub Repository](https://github.com/NVlabs/Sana)**  
**[Sana arXiv Paper](https://arxiv.org/abs/2410.10629)**

---

## **ABSTRACT**  
우리는 *Sana*라는 고해상도 text-to-image 생성 프레임워크를 소개합니다. 이 프레임워크는 4096×4096 해상도까지 이미지를 생성할 수 있습니다. 주요 혁신 사항은 다음과 같습니다:  
- **Deep Compression Autoencoder**: 이미지를 32배 압축하여 효율적인 처리를 가능하게 함.  
- **Linear DiT**: 고해상도에서 효율성을 높이기 위해 기존의 attention을 linear attention으로 대체.  
- **Decoder-only Small LLM as Text Encoder**: 경량화된 large language model을 통해 텍스트 이해력과 이미지-텍스트 alignment를 향상.  
- **Efficient Training and Inference Strategy**: 샘플링 단계를 줄이고 이미지 생성 속도를 개선하는 새로운 전략을 제안하여, Sana는 Flux-12B와 같은 대형 모델보다 100배 이상 빠른 성능을 보여줌.

---

## **1. INTRODUCTION**  
최근 latent diffusion models (LDM)의 발전으로 텍스트-이미지 생성 기술이 크게 진전되었습니다. 하지만 모델 크기와 복잡성의 증가(예: 0.6B에서 24B 파라미터 모델)로 인해 계산 비용이 매우 높아졌습니다. Sana는 cutting-edge 기술을 결합하여 고해상도 이미지 생성을 제공하면서도 계산 효율성을 높이고, 노트북 GPU와 같은 edge devices에서도 실행 가능한 솔루션을 제시합니다. 철학적으로, 이 논문은 고품질 이미지 생성을 democratizing하고 더 많은 사용자가 접근할 수 있도록 하는 한 걸음으로 볼 수 있습니다.

---

## **2. METHODS**

### **2.1 DEEP COMPRESSION AUTOENCODER**

#### **2.1.1 PRELIMINARY**  
전통적인 diffusion 모델(예: PixArt, Flux)은 이미지를 8배 압축하는 autoencoder를 사용했습니다. Sana는 32배 압축을 적용하여 latent token 크기를 줄여 고해상도 이미지(예: 4K)를 효율적으로 학습할 수 있게 합니다.

#### **2.1.2 AUTOENCODER DESIGN PHILOSOPHY**  
Sana의 autoencoder 설계는 압축과 품질의 균형을 맞추는 데 중점을 둡니다.  
- **Philosophy**: 고해상도 이미지를 압축하여 redundant 정보를 제거하면서도 중요한 세부 사항을 유지하는 것이 효율적인 이미지 생성을 위해 필수적입니다. 이 논문에서는 autoencoder가 압축을 완전히 담당하고, diffusion model은 잡음 제거에 집중하는 것이 최선의 설계라고 주장합니다.  
- **Philosophical Insight**: autoencoder와 diffusion model 간의 역할 분담은 AI 설계의 modular 접근 방식을 반영합니다. 각 구성 요소가 특정 작업을 전문적으로 처리함으로써 시스템 전체의 성능을 최적화하는 것이 핵심입니다.

#### **2.1.3 ABLATION OF AUTOENCODER DESIGNS**  
패치 크기와 채널 구성을 실험한 결과, autoencoder가 압축을 집중적으로 처리하게 하여 diffusion model이 잡음 제거에만 집중하도록 하는 것이 최적의 선택임을 확인했습니다.  
- **Philosophical Insight**: 각 구성 요소가 자신의 역할에 집중할 때 시스템의 성능이 향상된다는 점에서, 이 설계는 AI 시스템에서 전문화의 중요성을 다시 한 번 강조합니다.

- **주요 발견 사항**: 
  - 더 큰 패치 크기(예: AE-F32)가 고압축에 더 적합합니다.
  - 채널 크기 32가 속도와 품질의 균형을 가장 잘 맞춥니다.

---

### **2.2 EFFICIENT LINEAR DIT DESIGN**

#### **Linear Attention Block**  
기존의 attention은 계산 복잡도가 O(N²)로, 고해상도 이미지 처리 시 비효율적입니다. 이를 해결하기 위해 **linear attention**을 도입하여 계산 복잡도를 O(N)으로 줄였습니다. 이 접근 방식은 특히 고해상도 이미지 생성 작업에서 매우 효과적입니다.  
- **Philosophical Insight**: 사각형 attention에서 linear로 전환하는 것은 품질을 희생하지 않고 확장 가능성을 추구하는 AI 철학과 일치합니다.

#### **Mix-FFN Block**  
3×3 depth-wise convolution을 MLP 레이어에 통합하여 Mix-FFN 블록은 로컬 정보를 더욱 효과적으로 캡처할 수 있게 합니다. 이는 linear attention의 로컬 정보 처리 능력이 약한 문제를 보완합니다.  
- **Philosophical Insight**: 이 설계는 신경망에서 로컬 처리와 글로벌 처리를 결합하는 중요성을 보여줍니다. 이는 인간이 시각 정보를 처리하는 방식과 유사합니다.

#### **DiT without Positional Encoding (NoPE)**  
Sana는 positional encoding을 제거하고도 성능 손실이 없음을 보여줍니다. 이는 depth-wise convolution이 암묵적으로 위치 정보를 포함하기 때문입니다.  
- **Philosophical Insight**: 이 발견은 positional encoding이 항상 필요하지 않다는 점을 제기하며, AI가 더 암묵적이고 유연한 표현을 지향하는 추세와 일치합니다.

#### **Triton Acceleration for Training and Inference**  
Triton을 사용해 linear attention 블록의 학습 및 추론 속도를 더욱 높였습니다. 커널 연산을 융합하여 데이터 전송과 관련된 오버헤드를 줄임으로써 더 빠른 처리 속도를 구현했습니다.

---

### Sana의 개요

![Sana Overview](/assets/images/posts/SanaOverview.png)

**그림 5: Sana의 개요**  
(a) 그림은 우리의 *Sana*의 고수준 학습 파이프라인을 설명합니다. 이 파이프라인은 32배 딥 압축 오토인코더, 선형 DiT, 복잡한 사용자 지시어를 포함하고 있습니다. 이 프레임워크에서는 positional embedding이 필요하지 않습니다.  
(b) 그림은 선형 DiT에서 사용된 선형 어텐션과 Mix-FFN 모듈의 세부 설계를 설명합니다.

---

### PyTorch 코드로 구현

#### **Deep Compression Autoencoder**

Sana의 32배 압축 오토인코더는 이미지 데이터를 잠재 공간으로 압축하는 역할을 합니다. 아래는 간단한 오토인코더 구조를 PyTorch로 구현한 예시입니다.

```python
import torch
import torch.nn as nn

class AutoEncoder(nn.Module):
    def __init__(self):
        super(AutoEncoder, self).__init__()
        # 인코더: 32배 압축
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1), # 더 깊은 압축을 위해 1024 채널
            nn.ReLU(True)
        )
        
        # 디코더: 원래 이미지 복원
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()  # 이미지 데이터는 0과 1 사이로 출력
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# 모델 초기화
autoencoder = AutoEncoder()
```

#### **Linear Attention Module**

Sana는 선형 어텐션을 사용하여 계산 복잡도를 O(N²)에서 O(N)으로 줄였습니다. ReLU를 활용한 선형 어텐션은 메모리 효율을 높입니다. 아래는 PyTorch로 구현한 선형 어텐션 블록입니다.

```python
class LinearAttention(nn.Module):
    def __init__(self, embed_dim):
        super(LinearAttention, self).__init__()
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)
        
        # ReLU 기반 선형 어텐션
        attention = torch.relu(Q @ K.transpose(-2, -1))  # 쿼리와 키의 내적
        attention = attention / attention.sum(dim=-1, keepdim=True)  # 정규화
        out = attention @ V  # 어텐션 가중치를 값에 적용
        
        return out

# 예시: 배치 크기 32, 시퀀스 길이 10, 임베딩 차원 64
input_tensor = torch.rand(32, 10, 64)
linear_attention = LinearAttention(embed_dim=64)
output = linear_attention(input_tensor)
```

#### **Mix-FFN (Feed-Forward Network)**

Mix-FFN은 깊이별 컨볼루션을 포함하여 로컬 정보를 더 잘 캡처하도록 설계되었습니다. 아래는 1x1과 3x3 컨볼루션을 사용하는 PyTorch 기반 FFN 블록입니다.

```python
class MixFFN(nn.Module):
    def __init__(self, embed_dim):
        super(MixFFN, self).__init__()
        self.conv1 = nn.Conv2d(embed_dim, embed_dim, kernel_size=1)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.conv3 = nn.Conv2d(embed_dim, embed_dim, kernel_size=1)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.conv3(x)
        return x

# 예시: 배치 크기 32, 채널 64, 이미지 크기 32x32
input_tensor = torch.rand(32, 64, 32, 32)
mix_ffn = MixFFN(embed_dim=64)
output = mix_ffn(input_tensor)
```

여기 논문을 단락별로 상세하게 정리한 내용입니다. Notion 포맷으로 구성하며, 딥러닝 네트워크 관련 철학적 논의도 포함했습니다.

---

### **2.3 TEXT ENCODER DESIGN**

#### **Why Replace T5 with Decoder-only LLM as Text Encoder?**  
최신 LLM들은 주로 decoder-only GPT 아키텍처를 기반으로 하며, 대규모 데이터로 학습되어 있습니다. T5와 비교할 때, 이러한 모델들은 강력한 추론 능력을 갖추고 있으며, 복잡한 인간의 지시를 Chain-of-Thought(CoT) 방식으로 따를 수 있습니다. 또한, Gemma-2와 같은 소형 LLM은 대형 모델과 유사한 성능을 발휘하면서도 훨씬 효율적입니다. 따라서 우리는 Gemma-2를 text encoder로 채택했습니다.  
- **철학적 배경**: 인간 지시에 대한 AI의 응답 능력을 강화하고, 복잡한 맥락 학습을 통한 텍스트-이미지 간의 정렬을 최적화하려는 시도입니다. 이는 AI가 단순한 기계적 처리를 넘어 인간의 의도를 깊이 이해하는 쪽으로 나아가는 방향성을 반영합니다.

#### **Stabilize Training using LLM as Text Encoder**  
Decoder-only LLM을 text encoder로 사용할 때 발생하는 불안정성을 해결하기 위해 텍스트 임베딩에 RMSNorm을 추가해 분산을 1로 정규화합니다. 또한, 작은 학습 가능한 스케일 팩터(예: 0.01)를 초기화하여 텍스트 임베딩에 곱하는 기법도 적용하여 수렴 속도를 가속화했습니다.  
- **철학적 배경**: 안정된 학습 과정을 위해 복잡한 모델의 출력값을 제어하는 것은 신경망의 수렴 속도를 높이고, 더 나은 학습 안정성을 보장하는 중요한 방법론입니다.

#### **Complex Human Instruction Improves Text-Image Alignment**  
Gemma 모델은 T5보다 더 나은 지시 따르기 능력을 가지고 있습니다. 이를 활용하여 복잡한 인간 지시(CHI)를 추가함으로써 텍스트-이미지 정렬을 개선할 수 있었습니다. 간단한 프롬프트에서도 CHI를 통해 더 안정적이고 정교한 이미지를 생성할 수 있습니다.  
- **철학적 배경**: 복잡한 지시를 통해 AI의 텍스트 처리 능력을 확장하는 것은 인간의 언어적 복잡성을 AI에 통합하려는 시도로 볼 수 있습니다. 이는 AI가 단순한 패턴 인식에서 인간과 상호작용할 수 있는 능력으로 발전하는 중요한 과정입니다.

---

### **3. EFFICIENT TRAINING/INFERENCE**

#### **3.1 DATA CURATION AND BLENDING**

##### **Multi-Caption Auto-labelling Pipeline**  
각 이미지에 대해 여러 VLM을 사용하여 캡션을 생성합니다. 이 접근 방식은 더 다양한 캡션을 자동 생성하고, 데이터 품질을 높이는 데 기여합니다.  
- **철학적 배경**: 다양한 캡션을 생성하는 것은 AI가 단일한 관점에 국한되지 않고, 보다 폭넓은 시각적 해석을 가능하게 하며, 이미지-텍스트 정렬의 정확성을 높입니다.

##### **CLIP-Score-based Caption Sampler**  
무작위로 캡션을 선택하는 대신, CLIP 점수를 기반으로 높은 품질의 캡션을 더 자주 샘플링합니다. 이는 학습 중 의미적 정렬을 개선하는 데 도움이 됩니다.  
- **철학적 배경**: 고품질 데이터를 우선적으로 학습시키는 것은 AI가 더 정확한 예측을 하게 만드는 핵심 전략이며, 데이터 품질이 중요한 역할을 함을 강조합니다.

##### **Cascade Resolution Training**  
기존 256px 사전 학습을 생략하고 512px부터 시작하여 1024px, 2K, 4K로 점차적으로 학습 해상도를 높입니다. 이를 통해 더 높은 해상도에서 세밀한 이미지-텍스트 정렬을 빠르게 학습할 수 있습니다.  
- **철학적 배경**: 해상도를 점진적으로 높이는 학습 방식은 점진적 성장을 통해 복잡한 문제를 해결하는 인간 학습의 자연스러운 과정과 유사합니다.

#### **3.2 FLOW-BASED TRAINING/INFERENCE**

##### **Flow-based Training**  
기존의 DDPM 방식에서 벗어나 Rectified Flow와 EDM을 사용하여 더 빠르고 안정적인 수렴을 가능하게 합니다. 이는 노이즈 예측에서 데이터 또는 속도 예측으로 전환하여 학습의 효율성을 높이는 방식입니다.  
- **철학적 배경**: 불확실성을 줄이기 위해 보다 안정적인 예측을 선호하는 인간의 추론 방식과 유사하게, AI도 더 안정적인 방식으로 예측을 개선하는 방향으로 발전하고 있습니다.

##### **Flow-based Inference**  
DPM-Solver++를 수정하여 Rectified Flow 공식을 적용한 Flow-DPM-Solver를 제안합니다. 이를 통해 더 적은 단계에서 높은 품질의 결과를 도출할 수 있었습니다.  
- **철학적 배경**: 효율적인 추론은 실용적인 AI 적용을 위해 필수적이며, 이는 인간이 실시간으로 문제를 해결하려는 속성에 대응하는 기술적 발전이라 할 수 있습니다.

---

### **4. ON-DEVICE DEPLOYMENT**  
우리는 INT8 양자화를 통해 모델을 경량화하여 엣지 디바이스에서의 성능을 향상시켰습니다. 또한, 성능 손실을 최소화하면서 실행 속도를 2.4배 가속화할 수 있었습니다.  
- **철학적 배경**: AI 모델이 고성능 서버뿐만 아니라 엣지 디바이스에서도 실행 가능하도록 경량화하는 것은 기술의 민주화와 접근성 확대를 위한 중요한 목표입니다.

---

### **5. EXPERIMENTS**

#### **Model Details**  
Sana-0.6B 모델은 590M 파라미터를 가지고 있으며, 성능과 효율성의 균형을 유지하는 최적의 아키텍처를 적용했습니다.

#### **Evaluation Details**  
우리는 FID, CLIP Score, GenEval, DPG-Bench 등 다섯 가지 주요 평가 지표를 사용하여 모델의 성능을 평가했습니다. 이는 텍스트-이미지 정렬 및 인간 선호도를 측정하는 데 중점을 둡니다.  
- **철학적 배경**: AI의 성능을 다각도로 평가하는 것은 단순한 정답 추출을 넘어, 인간의 인식과 맞물린 성능을 검증하려는 시도를 반영합니다.

---

### **6. RELATED WORK**  
이전 연구들은 텍스트-이미지 생성, 고해상도 이미지 생성, 모바일 장치로의 모델 배포 등을 다루고 있으며, Sana는 이러한 분야에서 효율성과 성능을 개선하는 방향으로 기여하고 있습니다.

---

### **7. CONCLUSION**  
Sana는 딥 압축 오토인코더, 선형 어텐션, 복잡한 인간 지시, 자동 캡션 생성, Flow 기반 학습 및 추론 기법을 통해 4096×4096 해상도에서 이미지를 빠르고 정확하게 생성하는 효율적인 파이프라인을 제시합니다. 향후 비디오 생성 파이프라인 구축을 목표로 삼고 있습니다.  
- **철학적 배경**: AI의 지속적인 발전을 통해 고해상도 이미지 생성뿐만 아니라 비디오와 같은 더 복잡한 문제를 해결하려는 방향성을 반영하고 있습니다.


---

이러한 최첨단 기술을 통합하여 *Sana*는 고해상도 이미지 생성의 도전 과제에 대한 강력하고 확장 가능한 솔루션을 제공하며, 압축, attention, 학습 전략의 균형을 통해 속도와 품질 모두를 최적화합니다. 철학적으로, 이는 AI의 modularity, 전문화, 그리고 효율적 확장이라는 원칙을 반영한 것이며, 딥러닝의 미래 혁신을 이끌 중요한 요소입니다.

---

## 최신 글
<ul>
  {% for post in site.posts %}
    <li>
      <a href="{{ post.url }}">{{ post.title }}</a>
      <span>{{ post.date | date: "%B %d, %Y" }}</span>
    </li>
  {% endfor %}
</ul>