---
title: "[논문리뷰] ConsiStory Review"
last_modified_at: 2024-10-17
categories:
  - 논문리뷰
tags:
  - ConsiStory
  - Text-to-Image
  - Diffusion Models
  - Consistent Generation
  - Training-Free
excerpt: "훈련 없이 일관된 텍스트-이미지 생성이 가능한 ConsiStory 모델 리뷰"
use_math: true
classes: wide
---

## Training-Free Consistent Text-to-Image Generation

YOAD TEWEL, NVIDIA, Israel and Tel Aviv University, Israel OMRI KADURI, Independent Scientist, Israel RINON GAL, NVIDIA, Israel and Tel Aviv University, Israel YONI KASTEN, NVIDIA, Israel LIOR WOLF, Tel Aviv University, Israel GAL CHECHIK, NVIDIA, Israel YUVAL ATZMON, NVIDIA, Israel

## Subject Description

"모자를 쓴 노인의 사진."

"아이에 대한 오래된 이야기 일러스트."

"갈색 눈을 가진 행복한 소녀의 하이퍼 리얼리즘 디지털 페인팅."

"공원을 걷는 중"

"칠판에 숫자를 쓰는 중"

"술집에서 맥주를 마시는 중" "학교 교복을 입고 있는 중"

<!-- 이미지 -->

"엄마와 함께 걷고 있는 중"

<!-- 이미지 -->

"헤드폰을 끼고, 고양이와 함께 있는 중"

<!-- 이미지 -->

Fig. 1. ConsiStory는 동일한 subject identity를 유지하고 제공된 텍스트에 맞는 일련의 이미지를 생성하는 방식으로 일련의 input prompts를 변환합니다. 또한 여러 subjects에 대한 일관된 identity를 유지할 수 있습니다. ConsiStory는 최적화나 pre-training을 필요로 하지 않습니다.

<!-- 이미지 -->

<!-- 이미지 -->

"책을 읽는 중"

<!-- 이미지 -->

"음식을 먹는 중"

<!-- 이미지 -->

<!-- 이미지 -->

"산에서 고양이와 하이킹 중"

<!-- 이미지 -->

<!-- 이미지 -->

Single Subject

Multi Subject

"고양이와 함께 시골에서 피크닉 중"

"눈 속에서, 헤드폰을 쓰고 있는 중"

<!-- 이미지 -->

"나무를 오르는 중"

<!-- 이미지 -->

"요리를 하는 중"

<!-- 이미지 -->

<!-- 이미지 -->

텍스트에서 이미지로 변환하는 모델들은 사용자에게 자연 언어로 이미지 생성 과정을 안내할 수 있는 새로운 창의적 유연성을 제공합니다. 그러나 다양한 prompts에서 동일한 주제를 일관되게 표현하는 데 어려움이 있습니다. 기존의 방법들은 특정 사용자가 제공한 주제를 설명하는 새로운 단어를 학습하거나 이미지 conditioning을 추가하여 모델을 미세 조정합니다. 이러한 방법들은 개별 주제에 대한 긴 최적화나 대규모 pre-training이 필요하며, 텍스트와 생성된 이미지를 일치시키는 데 어려움을 겪으며 여러 주제를 표현하는 데에도 어려움이 있습니다. 여기에서 우리는 ConsiStory라는 훈련이 필요 없는 접근법을 제안하여 사전 훈련된 모델의 내부 활성화를 공유하여 일관된 주제 생성을 가능하게 합니다. 우리는 subject consistency를 증진하기 위해 subject-driven shared attention block과 correspondence-based feature injection을 도입합니다.


between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines and demonstrate state-of-the-art performance on subject consistency and text alignment without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios and even enable training-free personalization for common objects.

Code will be available at our project page.

Additional Key Words and Phrases: Text-to-Image, Consistent, Story, Diffusion, Training-Free

## ACM Reference Format

Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. 2024. Training-Free Consistent Text-to-Image Generation. ACM Trans. Graph. 43, 4, Article 52 (July 2024), 18 pages. https://doi.org/10.1145/3658157

## 1 INTRODUCTION

대규모 텍스트-이미지 (T2I) diffusion 모델은 사용자가 텍스트로부터 상상력 넘치는 장면을 창조할 수 있게 해주지만, 그들의 확률적 성질로 인해 일관된 subject를 다양한 prompts에서 표현하는 데 어려움이 있습니다. 이런 일관성은 책과 이야기의 삽화부터 가상 자산의 디자인, 그래픽 소설, 합성 데이터 생성에 이르기까지 많은 응용 프로그램에서 필수적입니다.

일관된 이미지 생성을 위한 현재 접근법들 [Avrahami et al. 2023b; Feng et al. 2023; Jeong et al. 2023; Liu et al. 2023]은 주로 personalization에 의존합니다. 여기서 텍스트-이미지 모델은 특정 주제를 표현하기 위해 새로운 단어를 학습합니다. 그러나 이러한 personalization 기반 방법들은 몇 가지 단점이 있습니다. 첫째, 주제마다 훈련이 필요하고, 둘째, 동일한 이미지에서 여러 일관된 주제를 동시에 표현하는 데 어려움이 있으며, 셋째, 주제 일관성과 프롬프트 일치 사이에서 트레이드오프가 발생할 수 있습니다. Image-conditioned diffusion 모델을 훈련하는 대안들은 상당한 계산 자원이 필요하며, multi-object 장면으로 확장하기가 불확실합니다.

우리는 후처리 방식(a posteriori)의 한계를 피할 수 있음을 보여주며, 훈련 없이 zero-shot으로 일관성을 달성하는 방법을 제안합니다. 이를 위해, 생성 중간 단계에서 각 이미지가 서로 일관되게 생성될 수 있도록 diffusion 모델의 내부 특징 표현을 사용하여 alignment를 촉진합니다. 이렇게 하면 외부 소스와의 추가 정렬 없이 on-the-fly 일관된 생성을 가능하게 하며, 현재 state-of-the-art 방식보다 약 20배 빠르게 이미지를 생성할 수 있습니다.

우리의 접근법은 세 가지 단계로 구성됩니다. 먼저, 노이즈가 섞인 생성 이미지들에서 주제를 로컬라이즈합니다. 이후, 주제 일관성을 강화하기 위해 확장된 self-attention 메커니즘을 사용하여 각 이미지가 다른 프레임의 subject patches에 주의를 기울일 수 있도록 합니다. 이러한 과정은 일관된 주제를 얻는 데 기여하지만, layout diversity가 감소하는 문제를 유발할 수 있습니다. 이를 해결하기 위해, 비일관적인 샘플링 단계의 특징을 혼합하고, shared keys와 values에서 inference-time dropout을 적용하여 다양한 레이아웃을 유지합니다. 마지막으로, subject pixel 사이의 self-attention output features을 맞추어 세부적인 consistency를 향상시킵니다.

우리의 완성된 방법, ConsiStory는 이러한 구성 요소를 결합하여 훈련이 필요 없는 일관된 생성을 가능하게 합니다. 우리는 ConsiStory를 기존 접근법들과 비교하여, feature alignment를 생성 과정에서 수행함으로써 속도를 크게 높이면서 prompt alignment를 유지할 수 있음을 보여줍니다. 또한, 우리의 방법은 multi-subject 장면에도 적용 가능하며, 공통 object classes에 대한 훈련이 필요 없는 personalization도 가능합니다.

## 2 RELATED WORK

일관된 텍스트-이미지(T2I) 생성은 시각적으로 일관된 subjects를 묘사하는 일련의 이미지를 합성하는 작업입니다. 초기 연구는 personalization과 fine-tuning을 통해 일관성을 증진시켰습니다 [Gal et al. 2022; Ruiz et al. 2022]. [Jeong et al. 2023]은 특정 인물의 얼굴을 바꾸기 위해 personalization된 모델과 이미지 편집을 사용하였습니다.

[Gong et al. 2023]는 personalized LoRA 모델을 사용하여 multi-character 이미지를 반복적으로 생성하며, 텍스트에서 레이아웃으로 변환하는 모델의 사전 훈련이 필요합니다. [Feng et al. 2023; Liu et al. 2023]은 스토리보드 데이터셋을 활용하여 텍스트-이미지(T2I) 모델을 fine-tuning하며 이미지 프레임을 condition합니다. 이는 encoder 기반의 personalization 방식과 유사합니다(IP-Adapter [Ye et al. 2023] 및 ELITE [Wei et al. 2023]). 마지막으로, [Avrahami et al. 2023b]는 반복된 identity를 추출하여 생성된 이미지 세트를 통해 personalized LoRA 모델을 반복적으로 훈련합니다.

ConsiStory는 사전 훈련된 T2I 모델을 튜닝하거나 personalized하지 않고, 텍스트 프롬프트만을 사용하여 일관된 이미지를 생성합니다.

### Attention-based Consistency

비디오 분야에서는 temporal consistency를 증진하기 위해 self-attention 키와 값을 프레임 간에 공유하는 방법이 일반적입니다 [Wu et al. 2023]. 이는 비디오 생성이나 편집 작업에서 주로 사용됩니다 [Ceylan et al. 2023; Khachatryan et al. 2023; Wu et al. 2023]. 일부 연구는 일관된 identity를 inject하기 위해 소스 이미지의 attention 키와 값을 사용합니다 [Chang et al. 2023; Hu et al. 2023; Tu et al. 2023; Xu et al. 2023].

이미지 생성 분야에서는 텍스트 기반 편집 초기 연구들이 [Hertz et al. 2022; Parmar et al. 2023; Tumanyan et al. 2023] attention mask나 특징을 추출하여 이후 generation에서 이를 inject하여 구조를 유지하는 방법을 제안했습니다. 최신 연구들은 확장된 attention 메커니즘을 사용하여 이미지 레이아웃을 변경할 때 일관된 외형을 유지하는 것을 탐구했습니다 [Cao et al. 2023; Mou et al. 2023].

우리의 방법은 이러한 attention-sharing 아이디어에서 영감을 받았지만, 기존 이미지에서 특징을 가져오거나 전체 프레임을 align하지 않고 novel images 간에 subject-level 일관성을 유지하도록 개발되었습니다.

### Appearance Transfer using Dense Correspondence Maps

appearance transfer는 유사한 구조를 가진 이미지 간의 appearance를 전이하는 것으로 널리 연구되었습니다. [Liao et al. 2017]은 VGG 기반 맵을 사용하여 appearance를 전이했으며, [Benaim et al. 2020; Tumanyan et al. 2022]은 이미지-이미지 변환을 위해 이러한 매핑을 활용하는 generative 모델을 훈련했습니다. 최근 diffusion 모델은 이미지 간에 강력한 zero-shot correspondence를 설정하여 instance 교체, 이미지 편집, 강력한 registration 등의 응용이 가능함을 발견했습니다 [Hedlin et al. 2023; Luo et al. 2023b; Zhang et al. 2023a].

본 연구에서는 diffusion 기반 DIFT 맵을 활용하여 여러 이미지 간의 특징을 공유하고 일관된 appearance의 주제를 생성하도록 유도합니다. 이는 후처리로서 appearance transfer를 수행하는 것이 아니라 denoising 과정 전반에 걸쳐 특징을 align하는 방식입니다.

## 3 PRELIMINARIES: SELF-ATTENTION IN T2I MODELS

우리의 방법은 T2I diffusion 모델에서 self-attention 메커니즘을 조작합니다. 우리는 우선 self-attention의 작동 방식을 설명하고 주요 표기법을 소개합니다.

self-attention layer는 이미지 패치의 특징을 설명하는 일련의 토큰을 입력으로 받습니다. 각 토큰은 세 가지 self-attention 행렬을 통해 선형 투영됩니다: 𝑾 𝐾, 𝑾 𝑉, 𝑾 𝑄. 이 투영의 결과는 각각 "Keys", "Values", "Queries"로 불립니다.

자세히 설명하면, 생성된 배치에서 𝑖번째 이미지 항목을 고려할 때, 𝑥𝑖 ∈ ℝ𝑃 × 𝑑는 특징 차원 𝑑를 가진 𝑃개의 입력 토큰 벡터로 이루어진 시퀀스를 나타냅니다. 𝐾𝑖 = 𝑥𝑖 · 𝑾 𝐾, 𝑉𝑖 = 𝑥𝑖 · 𝑾 𝑉, 𝑄𝑖 = 𝑥𝑖 · 𝑾 𝑄로 정의합니다. self-attention 맵은 다음과 같이 주어집니다:

𝐴𝑖 = softmax (𝑄𝑖 𝐾𝑖⊤/√𝑑𝑘) ∈ ℝ𝑃 × 𝑃, (1)

여기서 𝑑𝑘는 𝑾 𝐾, 𝑾 𝑄 투영의 특징 차원입니다. 직관적으로 이 맵은 이미지 내 모든 패치 쌍 간의 연관성을 제공합니다. 이를 통해 "Value" 특징이 주어진 타겟 패치에 얼마나 영향을 미쳐야 하는지를 가중합니다.

이후, 우리는 이 self-attention 메커니즘에 개입하여 생성된 배치의 이미지들이 서로의 𝑥 𝑜𝑢𝑡 활성화에 영향을 줄 수 있도록 합니다.

## 4 METHOD

우리의 목표는 다양한 프롬프트에 걸쳐 일관된 subject를 묘사하는 일련의 이미지를 생성하는 것입니다. 이를 위해 우리는 T2I 모델의 이미지 denoising 동안 internal activation을 더 잘 align하는 방식을 제안합니다. 중요한 것은 추가 훈련 없이 inference 기반 메커니즘만을 사용하여 일관성을 강화하는 것입니다.

우리의 접근법은 세 가지 주요 구성 요소로 이루어집니다. 첫째, 우리는 subject-specific 정보를 공유하기 위해 subject-driven self-attention(SDSA) 메커니즘을 도입합니다. 둘째, SDSA 구성 요소가 생성된 레이아웃에서의 변이를 줄일 수 있기 때문에, 주의력 약화를 위한 dropout 메커니즘과 vanilla, non-consistent 샘플링 단계에서 얻은 query 특징 혼합을 통한 layout 다양성 유지 전략을 제안합니다. 셋째, 다양한 이미지 간 일관성을 개선하기 위해 feature injection 메커니즘을 추가로 도입합니다.

각 구성 요소는 아래에서 자세히 설명합니다.

### 4.1 Subject-driven self-attention

subject consistency를 증진하기 위한 간단한 아이디어로, 한 이미지의 query가 배치 내 다른 이미지의 key와 value에 주의를 기울이도록 self-attention을 확장하는 방법을 고려할 수 있습니다. 이는 비디오 생성과 편집에서 주로 사용되는 방법이며 [Wu et al. 2023], 프레임 간의 일관성을 증진합니다.

하지만, 우리의 경우 비디오와는 다릅니다. 비디오는 동일한 프롬프트로 여러 프레임이 생성되지만, 우리는 각 프레임이 고유한 프롬프트를 따르기를 원하며 배경과 레이아웃 다양성도 유지하고자 합니다. 이를 위해 subject-driven self-attention(SDSA) 메커니즘을 활용하여 동일한 subject 영역에만 주의를 집중하도록 하여 background features는 독립적으로 남겨둡니다.

subject-driven self-attention (SDSA) 메커니즘을 통해 각 이미지가 동일한 subject 영역에 주의를 집중하게 하여, 동일한 subject의 요소들 간의 특징이 공유되는 반면, background 특징은 독립적으로 유지됩니다. 이를 위해 우리는 이전 연구들 [Cao et al. 2023; Chefer et al. 2023]과 유사하게, subject token과 관련된 cross-attention 맵을 평균화하고 임계값을 설정하여 subject-specific 마스크를 생성합니다(자세한 내용은 부록 B 참조). 이 subject-specific 마스크를 활용하여 SDSA가 각 이미지에서 자기 패치 또는 다른 이미지의 subject 패치에만 주의를 기울일 수 있도록 합니다.

### 4.2 Enriching Layout Diversity

SDSA를 사용하면 prompt alignment가 복원되고 background collapse가 방지되지만, 여전히 이미지 레이아웃의 과도한 유사성이 발생할 수 있습니다. 예를 들어, subjects가 비슷한 위치와 포즈로 생성될 가능성이 큽니다.

결과물의 다양성을 높이기 위해 우리는 두 가지 전략을 제안합니다. 첫째, 비일관적인 샘플링 단계에서 얻은 특징을 통합하고, 둘째, SDSA를 dropout 메커니즘을 통해 약화시켜 다양한 레이아웃을 촉진합니다.

**Vanilla Query Features 활용:** 최근 연구 [Alaluf et al. 2023]에 따르면, diffusion 모델은 한 이미지의 외형과 다른 이미지의 구조를 결합할 수 있습니다. 우리는 다양한 포즈 변화를 위해 layout을 통제하는 초기 diffusion 단계에서 vanilla denoising을 통해 생성된 query 특징을 SDSA의 query 특징과 선형적으로 혼합하는 query-blending 메커니즘을 도입합니다.

**Self-Attention Dropout:** layout 다양성을 높이기 위한 두 번째 전략으로, SDSA를 dropout 메커니즘을 통해 약화시키는 방법을 제안합니다. 특정 denoising 단계에서 각 이미지의 subject 마스크 일부를 무작위로 0으로 설정하여 주의력 공유 강도를 약화시킴으로써 더 다양한 레이아웃을 유도합니다.

이 두 가지 메커니즘을 통해 비일관적인 샘플링에서 얻은 다양성을 유지하고, 과도한 일관성 문제를 방지하여 일관성을 크게 손상시키지 않고도 다양성을 증가시킵니다.

### 4.3 Feature Injection

공유된 attention 메커니즘은 subject 일관성을 개선하지만, 세밀한 시각적 특징에서는 subject의 identity에 부정적인 영향을 미칠 수 있습니다. 따라서 일관성을 추가로 개선하기 위해 새로운 cross-image Feature Injection 메커니즘을 제안합니다.

여기서는 각 이미지의 동일한 영역 간(예: 왼쪽 눈) 일관성을 높이기 위해 self-attention의 output 특징을 align합니다. 이를 위해, DIFT [Tang et al. 2023] 특징을 사용하여 각 이미지 쌍 간의 patch correspondence map을 작성하고, 가장 높은 유사성을 가진 patch를 선택해 특징을 혼합합니다.

이를 통해 다양한 이미지에서 subject appearance를 더 일관되게 유지하고, background는 영향을 받지 않도록 주제 마스크를 사용하여 feature injection을 적용합니다.

### 4.4 Anchor Images and Reusable Subjects

추가적인 최적화를 위해, 생성된 이미지 중 일부를 "anchor images"로 지정하여 연산 복잡도를 줄일 수 있습니다. anchor 이미지들만 key와 value를 공유하고, feature injection 또한 anchor 이미지에서만 수행합니다. 이는 일관성을 유지하면서 더 빠른 추론이 가능하게 하며, anchor 이미지를 기반으로 동일한 subjects를 새로운 장면에서도 재사용할 수 있게 합니다.

### 4.5 Multi-Subject Consistent Generation

personalization 기반 접근법은 단일 이미지 내에서 여러 주제의 일관성을 유지하는 데 어려움을 겪지만 [Gu et al. 2023; Kumari et al. 2022; Po et al. 2023], ConsiStory는 여러 subject-specific 마스크의 합집합을 사용하여 multi-subject 일관성 생성을 간단하게 구현할 수 있습니다. 여러 주제가 서로 의미적으로 다를 경우, 정보 누출 문제가 발생하지 않으며, ConsiStory는 이를 성공적으로 구현합니다.

## 5 EXPERIMENTS

우리는 ConsiStory를 기존 방법들과 비교하여 정성적 및 정량적 성능 평가를 수행했습니다. ConsiStory는 subject 일관성과 text alignment에서 우수한 성능을 보여주었으며, 다양한 프롬프트에 대한 consistency를 유지하면서 prompt-alignment도 높게 유지했습니다.

**사용자 연구 결과:** 사용자 연구에서도 ConsiStory가 높은 subject consistency와 prompt alignment로 타 방법들보다 선호도가 높게 나타났습니다.

**Runtime Comparison:** ConsiStory는 SoTA 방식보다 약 25배 빠른 속도로 일관된 subject 생성을 구현하였으며, H100 GPU에서 32초의 시간으로 앵커 이미지와 새로운 프롬프트에 대한 이미지를 생성할 수 있었습니다.

## 6 LIMITATIONS

ConsiStory는 몇 가지 한계점을 가지고 있습니다. 첫째, subject localization은 cross-attention 맵에 의존하므로, 비정상적인 스타일에서는 localization에 실패할 수 있습니다. 둘째, 외형과 스타일을 분리하는 데 어려움을 겪으며, 동일한 스타일을 공유하는 경우에만 일관된 생성을 할 수 있습니다.

## 7 CONCLUSIONS

우리는 ConsiStory라는 훈련이 필요 없는 접근법을 제안하여, 사전 훈련된 텍스트-이미지 diffusion 모델을 사용해 시각적으로 일관된 subject 생성을 구현했습니다. ConsiStory는 타 방법들보다 20배 빠르며, prompt alignment를 훨씬 잘 유지할 수 있습니다.