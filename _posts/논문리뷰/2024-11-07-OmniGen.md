---
title: "[논문리뷰] OmniGen Review"
last_modified_at: 2024-10-17
categories:
  - 논문리뷰
tags:
  - OmniGen
  - Unified Image Generation
  - Diffusion Models
  - Consistent Generation
  - Multi-Modal
excerpt: "멀티모달 통합 이미지 생성을 지원하는 OmniGen 모델 리뷰"
use_math: true
classes: wide
---

## OmniGen: Unified Image Generation

Shitao Xiao $^{∗}$, Yueze Wang$^{∗}$, Junjie Zhou$^{∗}$, Huaying Yuan$^{∗}$, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, Zheng Liu † Beijing Academy of Artificial Intelligence {stxiao, yzwang}@baai.ac.cn, zhengliu1026@gmail.com

Figure 1: OmniGen은 단일 프레임워크 내에서 다양한 이미지 생성 작업을 수행할 수 있는 능력을 보여줍니다. 또한 추론 능력과 맥락 학습 기능을 가지고 있습니다.

<!-- image -->

$^{∗}$Co-first authors

## Abstract

대규모 언어 모델(Large Language Models, LLMs)의 출현은 언어 생성 작업을 통합하고 인간-기계 상호작용을 혁신했습니다. 그러나 이미지 생성 분야에서는 다양한 작업을 단일 프레임워크 내에서 처리할 수 있는 통합 모델이 거의 연구되지 않았습니다. 본 연구에서는 통합 이미지 생성을 위한 새로운 diffusion 모델인 OmniGen을 소개합니다. Stable Diffusion과 같은 인기 있는 diffusion 모델과 달리, OmniGen은 다양한 제어 조건을 처리하기 위해 ControlNet이나 IP-Adapter와 같은 추가 모듈이 필요하지 않습니다. OmniGen의 특징은 다음과 같습니다: 1) 통합: OmniGen은 텍스트-이미지 생성 기능을 시연할 뿐만 아니라 이미지 편집, 주제 기반 생성, 시각적 조건 기반 생성 등 다양한 다운스트림 작업을 기본적으로 지원합니다. 또한 가장자리 검출, 인간 포즈 인식과 같은 전통적인 컴퓨터 비전 작업을 이미지 생성 작업으로 변환하여 처리할 수 있습니다. 2) 단순성: OmniGen의 아키텍처는 매우 단순화되어 있으며 추가 텍스트 인코더가 필요하지 않습니다. 또한 기존의 diffusion 모델보다 사용자 친화적이어서 인간 포즈 추정 등의 추가 전처리 과정 없이 지시를 통해 복잡한 작업을 수행할 수 있습니다. 3) 지식 전이: 통합 형식에서 학습함으로써 OmniGen은 서로 다른 작업 간의 지식을 효과적으로 전이하며, 보지 못한 작업과 도메인도 처리할 수 있는 새로운 능력을 보여줍니다. 이 작업은 범용 이미지 생성 모델을 시도한 첫 사례이며, 여전히 해결되지 않은 문제가 남아 있습니다. 이 연구와 관련된 리소스는 https://github.com/VectorSpaceLab/OmniGen에서 오픈소스로 제공될 것입니다.

## 1 Introduction

Artificial General Intelligence (AGI)의 추구는 단일 프레임워크 내에서 다양한 작업을 처리할 수 있는 생성 기반 모델에 대한 수요를 증가시켰습니다. 자연어 처리(Natural Language Processing, NLP) 분야에서는 대규모 언어 모델(Large Language Models, LLMs)이 이 목표를 달성하는데 있어 모범 사례가 되었으며, 질문 응답, 텍스트 요약, 코드 생성과 같은 다양한 언어 작업에서 놀라운 유연성을 보여주고 있습니다.

하지만, 이미지 생성 분야에서는 이러한 범용성을 가진 모델이 아직 등장하지 않았습니다. 현재의 이미지 생성 모델은 특정 작업에서 능숙함을 보여줍니다. 예를 들어, 텍스트-이미지 생성 분야에서는 Stable Diffusion 시리즈 [56; 52; 13], DALL-E [55], Imagen [26]과 같은 최신 모델이 눈에 띄는 성과를 거두었습니다. 한편, diffusion 모델의 능력을 특정 작업에 맞추기 위해 ControlNet [73], T2i-Adapter [45]와 같은 추가 네트워크를 설계하여 텍스트-이미지 diffusion 모델에 시각적 조건을 지원하는 많은 노력이 이루어졌습니다. InstructPix2Pix [4]는 이미지 편집 작업에 맞춰진 종합 데이터셋으로 훈련되었습니다. 하지만 이러한 모델들은 특정 작업에 국한되어 있어 범용 시각 생성 모델로서의 포괄적인 인식 및 생성 능력을 보여주지 못합니다.

GPT가 언어 작업을 처리하는 것처럼, 텍스트-이미지 생성, 이미지 편집, 제어 가능한 생성, 이미지 복원과 같은 다양한 이미지 생성 작업을 단일 diffusion 프레임워크 내에서 해결할 수 있을까요? 이러한 범용 모델이 있다면, ControlNet, IP-Adapter, T2I-Adapter와 같은 추가 모듈의 필요성이 줄어들 수 있습니다. 이러한 가능성에 영감을 받아, 우리는 통합 이미지 생성 프레임워크인 OmniGen을 탐구합니다.

대중적인 diffusion 모델과 달리, OmniGen은 VAE와 transformer 모델의 두 가지 주요 구성 요소로 구성된 매우 간결한 구조를 가지고 있으며, 추가 인코더가 필요하지 않습니다. OmniGen은 텍스트와 이미지 입력을 조건으로 받아 이미지를 생성하며, 텍스트 전용 또는 이미지 전용 조건을 넘어서 혼합 입력 조건을 수용할 수 있습니다. 견고한 통합 모델을 훈련하기 위해 다양한 작업을 하나의 형식으로 통합한 첫 번째 대규모 통합 이미지 생성 데이터셋 X2I를 구축했습니다. 또한 인간 포즈 추정, 가장자리 검출, 이미지 디블러링과 같은 여러 전통적인 컴퓨터 비전 작업을 통합하여 모델의 기능 한계를 확장하고 복잡한 이미지 생성 작업에 대한 능력을 강화했습니다. 우리의 모델은 여러 벤치마크에서 평가되었으며, 기존 모델과 비교해 경쟁력 있는 텍스트-이미지 생성 능력을 보여주었습니다. 또한, OmniGen은 이미지 편집, 시각적 조건 기반 생성, 주제 기반 생성과 같은 다양한 이미지 생성 작업을 기본적으로 지원하며, 이는 현재의 diffusion 모델로는 불가능한 영역입니다. OmniGen의 설계는 다양한 시나리오에 대한 강력한 전이 학습을 가능하게 하며, 새로운 작업과 도메인을 처리할 수 있는 능력을 갖추고 있습니다. 우리의 기여는 다음과 같이 요약됩니다:

- · OmniGen은 다양한 영역에서 우수한 성능을 보이는 통합 이미지 생성 모델을 소개합니다. OmniGen은 경쟁력 있는 텍스트-이미지 생성 능력을 보여주며, 제어 가능한 이미지 생성 및 주제 기반 생성과 같은 다양한 다운스트림 작업을 기본적으로 지원합니다. 또한 전통적인 컴퓨터 비전 작업도 수행할 수 있습니다. OmniGen은 이러한 포괄적인 기능 수준을 달성한 최초의 이미지 생성 모델입니다.
- · "anything to image"를 의미하는 X2I라는 포괄적인 이미지 생성 데이터셋을 구축했습니다. 이 데이터셋은 다양한 이미지 생성 작업을 모두 통합된 형식으로 표준화했습니다.
- · 다중 작업 데이터셋에 대한 통합 훈련을 통해 OmniGen은 새로운 작업과 도메인에서도 지식을 적용하여 새로운 기능을 발휘할 수 있습니다. 또한, OmniGen은 일정한 수준의 추론 능력을 보여줍니다.

논문의 나머지 부분은 다음과 같이 구성되어 있습니다: Section 2에서는 모델 아키텍처를, Section 3에서는 데이터셋 구축 방법을 설명합니다. Section 4에서는 다양한 이미지 생성 작업에 대한 모델 성능을 제시합니다. Section 5에서는 모델의 새로운 능력과 추론 능력을 분석하고, 이미지 생성에서의 CoT(Chain of Thought) 메커니즘의 잠재적인 응용을 탐구합니다. Section 6에서는 모델의 현재 한계를 논의합니다. Section 7에서는 관련 연구를 검토합니다.

## 2 OmniGen

이 섹션에서는 모델 아키텍처 및 훈련 방법을 포함한 OmniGen 프레임워크의 세부 사항을 제시합니다.

## 2.1 Model Design

Principles . 현재 diffusion 모델은 일반적으로 텍스트-이미지 작업에만 제한되며, 보다 광범위한 다운스트림 이미지 생성 작업을 수행할 수 없습니다. 실제 응용에서 사용자는 종종 diffusion 모델의 능력을 확장하기 위해 추가 네트워크 구조를 설계하고 통합해야 하며, 모델이 매우 번거로워집니다. 더욱이, 이러한 추가 네트워크는 보통 작업별로 설계되며, 다른 작업을 위해서는 더 많은 네트워크를 설계하고 훈련해야 합니다. 이러한 문제를 해결하기 위해 OmniGen의 설계 원칙은 다음과 같습니다: 1). 보편성: 다양한 작업을 위해 모든 형태의 이미지와 텍스트 입력을 허용; 2). 간결성, 지나치게 복잡한 구조 디자인과 수많은 추가 구성 요소를 피함.

Network Architecture . Figure 2에 나타난 바와 같이, OmniGen 프레임워크는 Variational Autoencoder (VAE) [28]와 사전 훈련된 대형 transformer 모델로 구성된 아키텍처를 채택합니다.

OmniGen에서 VAE는 이미지에서 연속적인 시각적 특징을 추출하고, transformer 모델은 입력 조건에 따라 이미지를 생성합니다. 본 연구에서는 SDXL [52]의 VAE를 사용하며, 훈련 중에 고정합니다. 텍스트 처리를 위한 transformer 모델 초기화에는 Phi-3 [1]을 사용하여, 우수한 텍스트 처리 능력을 이어받습니다. 최신 diffusion 모델들이 조건 정보를 사전 처리하기 위해 추가 인코더(예: CLIP 텍스트 인코더 및 이미지 인코더)를 요구하는 것과 달리, OmniGen은 자체적으로 조건 정보를 인코딩하여 파이프라인을 크게 단순화합니다. 또한, OmniGen은 단일 모델 내에서 텍스트와 이미지를 공동으로 모델링하여 서로 다른 입력 조건에 대해 상호작용을 가능하게 합니다.

Input Format . 모델의 입력은 자유 형식의 텍스트와 이미지로 이루어진 멀티모달 인터리브 형식입니다. 텍스트는 Phi-3의 tokenizer를 사용하여 처리하며, 이미지의 경우 간단한 선형 레이어가 포함된 VAE를 사용하여 잠재 표현을 추출한 뒤 이를 플래튼하여 시각적 토큰 시퀀스로 변환합니다. [50]에 따라, 시각적 토큰에 주파수 기반 위치 임베딩을 적용하며, SD3 [13]와 동일한 방식으로 다양한 가로세로 비율의 이미지를 처리합니다. 또한, 각 이미지 시퀀스 앞뒤에 특별 토큰 "<img>" 및 "</img>"을 추가한 뒤 텍스트 토큰 시퀀스에 삽입합니다. 입력 시퀀스의 끝에는 timestep 임베딩 [50]을 추가합니다.

Attention Mechanism . 텍스트는 분리된 토큰으로 모델링이 가능하지만, 이미지는 전체로 모델링되어야 한다고 주장합니다. 따라서, LLM에서 일반적으로 사용되는 causal attention 메커니즘을 수정하여 양방향 attention을 통합하였습니다. 특히, 시퀀스의 각 요소에 causal attention을 적용하되, 각 이미지 시퀀스 내에서는 양방향 attention을 적용합니다. 이를 통해 각 패치가 동일 이미지 내의 다른 패치에 주목할 수 있으며, 각 이미지는 이전에 나타난 다른 이미지나 텍스트 시퀀스에만 주목할 수 있게 됩니다.

Inference . 추론 과정에서, 우리는 무작위로 Gaussian noise를 샘플링한 후 flow matching 방법을 적용하여 목표 속도를 예측하고, 여러 단계를 반복하여 최종 잠재 표현을 얻습니다. 마지막으로, VAE를 사용하여 잠재 표현을 예측된 이미지로 디코딩합니다. 기본 추론 단계는 50으로 설정됩니다. attention 메커니즘 덕분에, OmniGen은 kv-cache를 사용하여 이전 및 현재 키와 값 상태를 GPU에 저장함으로써 중복 계산 없이 attention을 계산할 수 있어 LLM처럼 추론 속도를 가속화할 수 있습니다.

## 2.2 Training Strategy

Train objective . 본 연구에서는 rectified flow [41]를 사용하여 모델의 파라미터를 최적화합니다. DDPM [25]과 달리, flow matching은 noise와 데이터를 직선으로 선형 보간하여 forward 과정을 수행합니다. timestep t에서 x$\_{t}$는 다음과 같이 정의됩니다:

x$\_{t}$ = t x + (1 - t ) ϵ ,

여기서 x는 원본 데이터이고, ϵ ∼ N (0 , 1)은 Gaussian noise입니다. 모델은 노이즈가 추가된 데이터 x$\_{t}$, timestep t 및 조건 정보 c가 주어졌을 때 목표 속도를 직접 회귀하도록 훈련됩니다. 구체적으로, 목표는 다음과 같이 평균 제곱 오차 손실을 최소화하는 것입니다:

L =$\_{E}$ $^{[}$| | ( x - ϵ ) - v$\_{θ}$ $^{(}$x$\_{t}$ , t, c $^{)}$| | 2 $^{]}$. (1)

이미지 편집 작업의 경우, 입력 이미지의 특정 영역을 수정하면서 다른 영역은 변경되지 않도록 하는 것이 목표입니다. 따라서 입력 이미지와 목표 이미지의 차이가 작아 모델이 입력 이미지를 그대로 출력하여 훈련 손실을 매우 낮게 만드는 지름길을 학습할 가능성이 있습니다. 이러한 현상을 완화하기 위해, 변화가 발생한 이미지 영역의 손실을 증폭합니다. 보다 구체적으로, 입력 이미지 x '와 목표 이미지 x의 잠재 표현에 기반하여 각 영역의 손실 가중치를 계산합니다:

w$\_{i,j}$ =$^{{}$ 1 if x$\_{i,j}$ = x ' i,j 1 || x - x $^{'}$| | 2 if x$\_{i,j}$ ̸ = x ' i,j (2)

이로 인해, 수정이 필요한 영역에 더 높은 가중치가 할당되어 모델이 수정해야 하는 영역에 집중하도록 유도합니다.

Training Pipeline . 이전 연구 [13; 18; 6]를 따라 훈련 과정에서 점진적으로 이미지 해상도를 높여갑니다. 낮은 해상도는 데이터 효율성을 높이며, 높은 해상도는 생성된 이미지의 미적 품질을 향상시킬 수 있습니다. 각 훈련 단계에 대한 자세한 정보는 Table 2.2에 나와 있습니다. 우리는 β = (0 . 9 , 0 . 999)를 사용하여 AdamW [42]를 최적화 도구로 채택합니다. 모든 실험은 104개의 A800 GPU에서 수행됩니다.

---

Table 1: OmniGen 훈련의 각 단계에 대한 자세한 정보.

|   Stage | Image Resolution   |   Training Steps (K) |   Batch Size |   Learning Rate |
|---------|--------------------|----------------------|--------------|-----------------|
|       1 | 256 × 256          |                  500 |         1040 |          0.0001 |
|       2 | 512 × 512          |                  300 |          520 |          0.0001 |
|       3 | 1024 × 1024        |                  100 |          208 |          4e-05  |
|       4 | 2240 × 2240        |                   30 |          104 |          2e-05  |
|       5 | Multiple           |                   80 |          104 |          2e-05  |

## 3 X2I Dataset

강력한 멀티태스크 처리 능력을 달성하기 위해 대규모이면서도 다양한 데이터셋을 기반으로 모델을 훈련하는 것이 필수적입니다. 그러나 이미지 생성 분야에서는 쉽게 접근할 수 있는 대규모이면서 다양한 데이터셋이 아직 등장하지 않았습니다. 본 연구에서는 "anything to image"라는 의미의 X2I 데이터셋이라는 대규모 통합 이미지 생성 데이터셋을 처음으로 구축하였습니다. 이 데이터는 통합 형식으로 변환되었으며, Figure 3에는 X2I 데이터셋의 몇 가지 예가 나와 있습니다. 전체 데이터셋은 약 1억 개의 이미지로 구성되어 있습니다. 이 데이터셋의 구성에 대한 자세한 설명은 다음 섹션에서 제공됩니다.

## 3.1 Text to Image

이 데이터 하위 집합의 입력은 단순 텍스트입니다. 여러 출처로부터 Recap-DataComp [35] (5600만 개 이미지의 하위 집합), SAM-LLaVA [6], ShareGPT4V [7], LAION-Aesthetic [58] (400만 개 이미지의 하위 집합), ALLaVA-4V [5], DOCCI [47], DenseFusion [36] 및 JourneyDB [60]와 같은 다양한 오픈 소스 데이터셋을 수집했습니다. 이러한 데이터셋의 양은 방대하지만, 이미지의 품질이 항상 높은 것은 아닙니다. 훈련 초기 단계에서는 이미지-텍스트 매칭 관계와 다양한 지식을 폭넓게 학습하기 위해 이들을 사용하며, 3단계 이후에는 고품질의 이미지 1600만 개로 구성된 내부 데이터 모음을 사용하여 생성 이미지의 미적 품질을 향상시킵니다. 많은 연구 [13; 6]에 따르면, 텍스트-이미지 모델을 대규모로 훈련할 때 생성된 세부 캡션이 성능을 크게 개선할 수 있음을 입증했습니다. 따라서 내부 데이터와 LAION-Aesthetic을 위해 InternVL2 [11]를 사용하여 내부 데이터에 대해 생성된 설명을 추가하였습니다.

## 3.2 Multi-modal to Image

대부분의 기존 diffusion 모델과 달리, 우리의 모델은 더 일반적이고 유연한 멀티모달 지시를 조건으로 받아 이미지를 생성할 수 있습니다.

### 3.2.1 Common Mixed-modal Prompts

이 부분의 데이터 입력은 자유롭게 혼합된 텍스트와 이미지입니다. 이미지 편집(SEED-Data-Edit [19], MagicBrush [72], InstructPix2Pix [4]), 인간 동작(Something-Something [23]), 가상 피팅(HR-VITON [31] 및 FashionTryon [75]), 스타일 전환(stylebooth [24])과 같은 다양한 작업과 소스로부터 데이터를 수집했습니다. 모든 작업을 Figure 3-(b)에 표시된 입력-출력 쌍 형식으로 표준화하였습니다.

정밀한 공간 제어를 위한 추가 시각 조건 사용 문제는 널리 주목받고 있습니다[73; 33]. 우리는 MultiGen [53] 데이터셋을 사용하여 이 기능을 학습하고, 여섯 가지 대표적인 시각 조건(Canny, HED, Depth, Skeleton, Bounding Box, Segmentation)을 선택했습니다.

Figure 3: OmniGen 모델의 훈련 데이터 예시. 모든 작업의 입력을 모델의 프롬프트로 사용되는 자유 형식의 이미지-텍스트 시퀀스로 표준화했습니다. placeholder |image\_i|는 프롬프트 내 i 번째 이미지의 위치를 나타냅니다.

---

## 3.2.2 Subject-driven Image Generation

우리는 주제 기반 이미지 생성을 위해 대규모의 기초 데이터셋(GRIT-Entity 데이터셋)과 고품질의 고급 데이터셋(Web Images 데이터셋)을 구축했습니다. GRIT-Entity 데이터셋의 경우, GRIT 데이터셋 [51]을 활용하여 이미지 내 객체 이름을 주석 처리했습니다. 이러한 주석을 바탕으로 Grounding DINO 모델 [40]을 사용하여 텍스트를 바운딩 박스로 연결했습니다. 바운딩 박스를 바탕으로 SAM [29]을 활용하여 잘린 이미지에서 객체 마스크를 획득했습니다. 이후 MS-Diffusion 모델 [64]을 사용하여 객체 이미지를 리페인팅하여 데이터 품질을 향상시켰습니다. 데이터 구축 과정과 최종 지시 형식은 Figure 4-(a)에 설명되어 있습니다. 이를 통해 600만 쌍의 데이터셋을 확보했습니다.

GRIT 기반 접근법은 상당한 양의 데이터를 제공하지만, 원본 이미지에서 직접 추출된 입력 데이터는 모델이 단순히 복사-붙여넣기 패턴에 빠질 위험이 있습니다. OmniGen의 주제 기반 이미지 생성 기능을 최대한 발휘하기 위해 잘 알려진 인물들의 자연스러운 이미지를 활용하여 고품질 웹 이미지 학습 데이터셋을 추가로 구축했습니다. 우선, Datacomp 데이터셋 [14]에서 Alt-text 항목 2천만 개를 샘플링하고, spaCy 3을 사용하여 명명된 개체 인식을 수행했습니다. 가장 빈번하게 언급되는 이름을 선택한 후 GPT-4o를 통해 실제로 잘 알려진 인물들을 필터링하여 2000개의 이름을 얻었습니다. 이후 처음 선택된 2000개 이름에 밀접하게 관련된 인물들을 추가하여 약 1만 개의 이름 쌍을 확장했습니다. 그런 다음, 검색 엔진에서 이러한 인물과 쌍의 이미지를 스크랩했습니다. 웹 이미지에는 특정 인물이 포함되지 않은 경우가 많기 때문에, InternVL [11]을 활용하여 단일 및 그룹 이미지를 교차 검증하는 전략을 설계했습니다. 이 과정을 통해 선별된 단일 및 그룹 이미지를 캡션하여 복장과 동작 등의 세부 정보를 추가했습니다. Figure 3-(c)에 제시된 것처럼, 우리는 53만 3000개의 이미지 쌍 데이터셋을 성공적으로 구축했습니다.

## 3.2.3 Computer Vision Tasks

OmniGen의 이미지 생성 기능을 향상시키기 위해 전통적인 컴퓨터 비전 작업을 도입했습니다. 저레벨 비전 작업(저조도 이미지 개선 [66], 비 내림 제거 [71], 디블러링 [46], 인페인팅 [53], 아웃페인팅 [53], 컬러라이제이션 [58])의 경우, 주석 자체가 이미지이므로 GPT-4o에서 무작위로 샘플링된 텍스트 지시만 추가했습니다. 고레벨 작업에서는 모든 주석을 이미지로 표현했습니다. 우리는 LAION [58]을 소스 이미지로, [53]의 주석을 타겟으로 하여 이미지 쌍(예: 원본 이미지와 해당 인간 포즈 매핑)을 구성했습니다. 주석에는 인간 포즈, 깊이 매핑, Canny, Segmentation이 포함됩니다. 또한, 이미지 세그멘테이션 참조를 위해 RefCOCO [27], ADE20k [76], ReasonSeg [30] 등 여러 데이터셋을 사용했습니다. Figure 3-(c)에 표시된 바와 같이, 입력은 소스 이미지와 자연어 표현이며, 출력은 해당 객체가 파란색으로 강조된 이미지입니다.

이러한 데이터셋을 구축하는 목적은 단순히 모델에 이러한 기능을 부여하는 것만이 아닙니다. 또한 전통적인 컴퓨터 비전 작업에서 습득한 지식을 이미지 생성 작업으로 전이하여 보다 정교한 이미지 생성 기능을 달성하는 것입니다. 우리의 실험에서도 멀티태스크 학습을 통해 모델이 새로운 능력을 발휘할 수 있음을 보여주었습니다.

## 3.3 Few-shot to Image

모델의 맥락 학습 기능을 자극하기 위해 Few-shot to Image 데이터셋을 구축했습니다. 구체적으로, 앞서 설명한 각 작업에 대해 몇 가지 예를 무작위로 선택하고, 원본 입력과 이 예를 결합하여 새로운 입력을 구성했습니다. Figure 3-(e)에서 구체적인 데이터 형식을 참고할 수 있습니다. 훈련 자원의 한계로 인해, 효율성을 높이기 위해 예시는 한 개만 사용했습니다.

---

## 4 Experimental Results

이 섹션에서는 OmniGen의 이미지 생성 작업과 전통적인 비전 작업에서의 성능 결과를 보여줍니다.

## 4.1 Image Generation

### 4.1.1 Qualitative Results

Figure 5는 텍스트-이미지 생성 작업의 결과를 보여줍니다. OmniGen이 텍스트 설명을 효과적으로 따르며 다양한 가로세로 비율의 이미지를 생성하는 것을 확인할 수 있습니다.

Figure 6은 주제 기반 생성 작업의 결과를 보여줍니다. 모델은 주어진 참조 이미지에서 필요한 객체를 추출하여 이에 따라 새로운 이미지를 생성할 수 있습니다. 또한, 참조 이미지에 여러 객체가 포함된 경우에도 텍스트 설명을 기반으로 필요한 객체(예: 그림의 고양이)를 선택하여 추가적인 전처리 없이 생성할 수 있습니다.

Figure 7은 멀티모달 지시에 따라 다양한 다운스트림 이미지 생성 작업을 수행한 결과를 요약합니다. 모델이 다양한 이미지 생성 작업을 능숙하게 처리할 수 있음을 보여줍니다.

### 4.1.2 Text to Image

[13]에 따라, 우리는 GenEval [22] 벤치마크에서 OmniGen의 텍스트-이미지 생성 능력을 평가했습니다. 우리 모델의 성능을 기존의 인기 있는 이미지 생성 모델들과 비교한 결과는 Table 2에 요약되어 있습니다. 놀랍게도, 우리의 모델은 최신 diffusion 모델인 SD3와 유사한 성능을 보여주며, 우리의 프레임워크의 효과성을 입증합니다. GenEval 벤치마크는 이미지의 미적 품질을 반영하지 않으므로, 이 부분은 향후 평가 항목으로 남겨두겠습니다.

**예제 설명**: 봄날 기차역 플랫폼에 서 있는 젊은 여성이 긴 파란색 트렌치코트와 흰 셔츠를 입고 있습니다. 그녀의 어두운 갈색 머리는 낮게 묶여 있으며, 몇 가닥의 머리카락이 바람에 날립니다. 기대에 찬 그녀의 눈이 햇빛을 받아 따뜻하게 빛나고 있습니다.

<!-- image -->

Figure 5: 텍스트-이미지 생성 작업의 예제. OmniGen은 다양한 가로세로 비율의 이미지를 생성할 수 있습니다.

---

### 4.1.3 Image Edit

Table 3은 EMU-Edit 테스트 데이터에서의 OmniGen 성능을 보여줍니다. 범용 모델로서 OmniGen은 최고의 전용 모델과 비교할 만한 성능을 입증합니다.

| Model               | CLIP-I ↑ | CLIP-T ↑ | DINO ↑ |
|---------------------|----------|----------|--------|
| InstructPix2Pix [4] | 0.834    | 0.219    | 0.762  |
| MagicBrush [72]     | 0.838    | 0.222    | 0.776  |
| PnP [62]            | 0.521    | 0.089    | 0.153  |
| Null-Text Inv. [44] | 0.761    | 0.236    | 0.678  |
| EMU-Edit [59]       | 0.859    | 0.231    | 0.819  |
| OmniGen             | 0.836    | 0.233    | 0.804  |

우리는 EMU-Edit [59] 데이터셋에서 다양한 이미지 편집 작업(배경 변경, 이미지 전체 수정, 스타일 변경, 객체 제거, 객체 추가, 지역 수정, 색상 및 질감 변경)에서 OmniGen을 다른 최신 모델들과 비교했습니다. 세 가지 메트릭을 측정했으며, CLIP-I와 DINO는 원본 이미지와 편집된 이미지 간 유사성을, CLIP-T는 편집된 이미지와 목표 캡션 간의 텍스트-이미지 유사성을 나타냅니다. OmniGen은 InstructPix2Pix [4] 모델보다 높은 성능을 보여주며, 현재 최고 성능 모델인 EMU-Edit [59]와 비슷한 성능을 보입니다.

### 4.1.4 DreamBooth

DreamBench [57]에서 단일 엔티티 주제 기반 생성 능력을 평가했습니다. DreamBench는 30개의 주제(예: 개와 장난감)에 대해 750개의 프롬프트를 포함하며, 각 프롬프트에 대해 4개의 이미지를 생성하여 총 3000개의 이미지를 평가 세트로 구성했습니다. DreamBooth 평가에서 DINO와 CLIP-I를 사용하여 주제 충실도를, CLIP-T를 사용하여 텍스트 충실도를 측정했습니다. Table 4는 결과를 요약한 것입니다. Fine-tuning을 기반으로 한 모델과 비교했을 때, OmniGen은 유사한 수준의 텍스트 충실도를 유지하면서도 주제를 더 잘 보존합니다.

| Model                  | DINO ↑  | CLIP-I ↑ | CLIP-T ↑ |
|------------------------|---------|----------|----------|
| Textual Inversion [15] | 0.569   | 0.780    | 0.255    |
| DreamBooth [57]        | 0.668   | 0.803    | 0.305    |
| BLIP-Diffusion [32]    | 0.670   | 0.805    | 0.302    |
| Re-Imagen [8]          | 0.600   | 0.740    | 0.270    |
| SuTI [10]              | 0.741   | 0.819    | 0.304    |
| Kosmos-G [49]          | 0.694   | 0.847    | 0.287    |
| OmniGen                | 0.801   | 0.847    | 0.301    |

### 4.1.5 Visual Conditional Controls

이미지 기반 프롬프트는 diffusion 모델에 대한 세부적인 공간 조건 제어를 제공합니다. OmniGen의 이 능력을 평가하기 위해 [33]에서 ADE20K 테스트 데이터셋(세그멘테이션 마스크 조건)과 MultiGen-20M 평가 세트(캔니 엣지 맵, HED 엣지 맵, 깊이 맵 조건)를 사용했습니다. 각 조건에 대해, 생성된 이미지와 입력 조건 간의 유사성을 측정하여 조건 제어 가능성을 평가했습니다. Table 5에 실험 결과가 요약되어 있습니다. OmniGen은 세그멘테이션 마스크와 HED 엣지 맵 조건에서 최적의 결과를 보여주었으며, 캔니 엣지 맵과 깊이 맵 조건에서도 경쟁력 있는 결과를 얻었습니다.

| Model                | Seg. Mask (mIoU ↑ ) | Canny Edge (F1 Score ↑ ) | Hed Edge (SSIM ↑ ) | Depth Map (RMSE ↓ ) |
|----------------------|---------------------|--------------------------|--------------------|----------------------|
| T2I-Adapter [45]     | 12.61              | 23.65                    | -                  | 48.4                 |
| Gligen [37]          | 23.78              | 26.94                    | 0.5634             | 38.83                |
| Uni-ControlNet [74]  | 19.39              | 27.32                    | 0.6910             | 40.65                |
| UniControl [53]      | 25.44              | 30.82                    | 0.7969             | 39.18                |
| ControlNet [73]      | 32.55              | 34.65                    | 0.7621             | 35.9                 |
| ControlNet++ [33]    | 43.64              | 37.04                    | 0.8097             | 28.32                |
| OmniGen              | 44.23              | 35.54                    | 0.8237             | 28.54                |

## 4.2 Computer Vision Tasks

Figure 8은 OmniGen이 전통적인 다양한 비전 작업을 수행한 정성적 결과를 보여줍니다. OmniGen은 비 내림 제거, 디블러링, 인페인팅 등 저레벨 비전 작업을 처리할 수 있습니다. Figure 8 하단에서는 OmniGen이 인간 포즈 인식 및 깊이 추정과 같은 고레벨 작업도 처리할 수 있음을 볼 수 있습니다.

OmniGen은 시각 조건을 기반으로 이미지를 생성하는 것 외에도, 원본 이미지로부터 시각 조건을 추출하는 기능도 갖추고 있습니다. 이로 인해 우리는 OmniGen이 단순히 참조 이미지로부터 조건 정보를 추출하고 이를 바탕으로 새로운 이미지를 생성할 수 있는지 탐구하게 되었습니다. 놀랍게도, 별도의 전처리 단계 없이도 OmniGen은 이를 훌륭하게 수행해냅니다. Figure 9에 나타난 바와 같이, 기존의 ControlNet 작업 흐름은 참조 이미지에서 조건 정보를 추출하고 이를 이미지 생성에 반영하기 위해 여러 네트워크 구성 요소와 연산이 필요합니다. 하지만 OmniGen은 참조 이미지와 텍스트 지시를 단일 단계로 처리하여 새로운 이미지를 생성할 수 있습니다.

---

## 5 Further Analysis

LLMs은 새로운 작업과 도메인에서 뛰어난 일반화 능력을 보여주며, 맥락 학습 및 체인 오브 사고(Chain of Thought, CoT) 메커니즘과 같은 기능을 통해 성능을 향상시킬 수 있습니다. OmniGen에서도 유사한 기능을 관찰했으며, 이 섹션에서 우리의 발견을 제시합니다.

### 5.1 Emerging Capabilities

모든 작업을 통일된 형식으로 표준화하고 X2I 데이터셋에서 훈련함으로써 OmniGen은 범용적인 지식을 획득하고, 서로 다른 시나리오와 작업 간의 지식 전이가 가능해져 새로운 작업과 도메인에 대한 생성 능력을 발휘할 수 있습니다. 다음 예를 통해 여러 새로운 능력을 설명합니다.

#### Task Composition
실제 응용에서는 사용자 요구가 여러 작업의 조합으로 이루어지는 경우가 많습니다. Figure 10-(a)에서 보이는 바와 같이, 우리의 모델은 이미지 인페인팅과 같은 다양한 작업에 대한 다중 지시(예: "머리를 하얀색으로 바꾸기")를 동시에 처리할 수 있으며, 동일한 작업에 대한 다중 지시(예: "남성의 얼굴에 선글라스를 추가하고 옷 색을 파란색으로 변경하기")도 수행할 수 있습니다. 이러한 결과는 OmniGen의 다양한 활용 가능성과 실전에서의 잠재력을 보여줍니다.

#### Implicit Combination of Tasks
명시적인 작업 조합 외에도, 우리의 모델은 단일 지시를 통해 여러 작업을 암시적으로 수행할 수 있습니다. Figure 9에서 볼 수 있듯이, "이 인간 포즈/깊이 매핑을 따라 이미지를 생성하라"라는 입력을 받으면, 모델은 참조 이미지에서 관련 조건 정보를 추출하고 이를 기반으로 새로운 이미지를 생성할 수 있습니다. 이 과정은 모델 내부에서 모든 처리가 완료되며, 사용자는 단순한 명령만 입력하면 되므로, 기존의 ControlNet과 같은 시스템에서 필요로 하는 조건 추출 및 입력 단계가 필요 없습니다.

#### In-context Learning for Unseen Tasks and Domains
Figure 10-(b)에 나타난 예제에서 볼 수 있듯이, 모델이 훈련 중 접하지 않았던 작업에서도 예제를 제공함으로써 모델이 성공적으로 해당 작업을 수행할 수 있습니다. 예를 들어, 스크립블 데이터를 기반으로 이미지를 생성하는 작업은 훈련 중 접하지 않았지만, 예제를 제공하면 모델이 정확한 예측을 할 수 있습니다. 이는 OmniGen의 맥락 학습 기능이 새로운 작업과 도메인에 대한 일반화 능력을 향상시킬 수 있음을 보여줍니다.

#### End-to-end Workflow
사용자는 일반적으로 여러 모델을 로드하고 다단계 처리를 수행해야 만족스러운 이미지를 생성할 수 있으며, 이러한 워크플로우는 매우 번거롭고 비용이 많이 듭니다. 이로 인해 ComfyUI$^{4}$와 같은 오픈 소스 도구와 파이프라인이 개발되었습니다. 우리의 모델은 우수한 멀티모달 이해와 이미지 생성 능력을 갖추고 있어, 외부 모델에 의존하지 않고 많은 작업을 완료할 수 있습니다. 예를 들어, Figure 6에서 보듯이 사용자는 다중 요소를 포함한 이미지에서 특정 객체를 텍스트 지시로 지정("|image\_2|에 있는 고양이")할 수 있으며, 이러한 지시를 기반으로 새 이미지를 생성할 수 있습니다.

### 5.2 Reasoning Ability

모델의 추론 능력을 탐구하여 Figure 11에 그 결과를 제시했습니다. Figure 11 왼쪽에서는 명시적으로 특정 객체를 지정하지 않은 "어디에서 손을 씻을 수 있을까요? |image\_1|에서 찾아주세요"와 같은 지시를 주었을 때, 모델은 이미지를 인식하고 세면대가 필요한 상황임을 추론합니다. 그 결과, 모델은 이미지에서 세면대 영역을 식별하고 이를 강조합니다. 이 기능은 다중 모달 지시를 이해하고 필요한 객체를 찾고 후속 작업을 계획하는 데 도움을 줄 수 있는 인공지능 에이전트 개발에 잠재적인 응용 가능성을 제공합니다. 또한, Figure 11 오른쪽에서는 모델이 목표 객체를 추론한 후 편집 작업을 수행할 수도 있음을 보여줍니다. 만약 일치하는 객체가 없으면, 모델은 관련 없는 객체를 수정하지 않습니다.

### 5.3 Chain of Thought

체인 오브 사고(CoT) 방식은 작업을 여러 단계로 분해하여 순차적으로 해결함으로써 LLM의 성능을 크게 향상시킬 수 있습니다. 유사한 접근을 이미지 생성에 적용할 수 있는지에 대해 연구했습니다. 인간의 그림 그리는 방식을 참고하여 단계별로 점진적으로 이미지를 수정하고 세밀한 작업을 수행하는 과정을 시뮬레이션하려 합니다.

Figure 12에서는 점진적으로 이미지가 완성되는 과정을 나타내는 일러스트레이션이 나와 있습니다. 이는 사용자가 이미지 생성 과정에 좀 더 능동적으로 참여할 수 있게 하여, 최종 이미지가 나오기까지 기다리는 것이 아닌 중간 결과를 조정할 수 있는 새로운 가능성을 열어줍니다. 다만, 최종 생성된 이미지의 품질이 기존 모델을 뛰어넘지는 못했습니다. 단계별 생성 방식에서는 때때로 잘못된 수정이 포함되어 최종 이미지에 혼란을 초래할 수 있습니다. 이는 접근법이 비현실적이라는 것을 의미하지 않으며, 현재는 예비 탐색 단계에 그치고 있어 추가 최적화가 필요합니다.

---

## 6 Limitations and Discussions

OmniGen의 현재 모델은 다음과 같은 몇 가지 한계점을 가지고 있으며, 이에 대한 논의는 향후 연구의 방향을 제시합니다.

- **텍스트 프롬프트에 대한 민감도**: 기존의 diffusion 모델들과 유사하게, OmniGen은 텍스트 프롬프트에 매우 민감합니다. 일반적으로 텍스트 설명이 구체적일수록 더 높은 품질의 이미지를 생성할 수 있습니다.
  
- **텍스트 렌더링 제한**: 현재 모델은 짧은 텍스트만 처리할 수 있으며, 긴 텍스트를 정확하게 생성하지 못합니다. 또한, 훈련 중 입력 이미지의 수가 최대 세 개로 제한되었기 때문에 긴 이미지 시퀀스를 처리하는 데 어려움이 있습니다.

- **세부 사항 오류**: 생성된 이미지에는 때때로 작은 세부 사항 오류가 포함되며, 특히 손과 같은 섬세한 부분에서 문제가 발생합니다. 주제 기반 생성 작업에서는 얼굴의 특정 특징이 원본 이미지와 일치하지 않는 경우가 종종 발생합니다. OmniGen은 또한 손의 묘사에서 종종 잘못된 세부 사항을 생성합니다.

- **새로운 이미지 타입 처리 불가**: OmniGen은 Surface Normal과 같은 새로운 이미지 유형을 처리할 수 없습니다. 

대부분의 한계는 관련 데이터를 추가하여 모델을 훈련함으로써 해결할 수 있을 것으로 예상됩니다. 또한, 대부분의 모델과 비교했을 때, OmniGen은 다양한 이미지 생성 작업을 기본적으로 지원하므로 추가 네트워크를 구축하는 데 드는 비용 없이 다운스트림 작업을 위해 손쉽게 파인튜닝할 수 있습니다.

## 7 Related Work

### 7.1 Generative Foundation Models

생성형 기초 모델은 많은 현대 인공지능 시스템의 핵심 역할을 수행하며, 기계가 인간과 상호작용하는 방식을 혁신하고 있습니다. GPT 시리즈 [54; 48]는 대규모 데이터셋에서 학습을 통해 언어 모델이 여러 작업을 수행할 수 있음을 보여주었습니다. 이러한 흐름을 따라 대형 언어 모델(LLMs) [43; 3; 1]의 발전은 질문 응답, 텍스트 요약, 코드 생성과 같은 다양한 작업을 단일 프레임워크 내에서 처리하는 다재다능함을 더욱 부각시켰습니다. 언어를 넘어, 시각 및 언어 기능을 통합한 멀티모달 대형 언어 모델(Multi-modal LLM) [39; 12]도 제안되었습니다. 예를 들어, LLaVA [39]는 비전 인코더를 LLM과 연결하여 시각적 인식 및 이해 기능을 제공하는 대표적인 아키텍처입니다. 그러나 이러한 모델들은 텍스트와 이미지 입력을 혼합하여 처리할 수 있는 능력은 있지만 이미지를 생성하는 능력은 부족합니다. OmniGen은 임의의 인터리브 멀티모달 입력을 받아 이미지를 생성하는 범용 기초 모델 구축에 새로운 발걸음을 내디뎠습니다.

최근 일부 연구에서는 텍스트와 이미지 생성을 모두 지원하는 통합 모델을 탐구해 왔습니다. 예를 들어, Chameleon [61]에서는 이미지와 텍스트를 모두 토큰 시퀀스로 변환하여 이산 확률적 모형으로 모델링하였습니다. 동시에 TransFusion [77]과 Show-O [69]는 diffusion과 autoregressive 방법을 하나의 모델로 통합하여 텍스트를 autoregressive하게 생성하고 이미지를 diffusion 방식으로 생성하였습니다. 그러나 이러한 모델들은 대부분 텍스트-이미지 생성 작업만 수행할 수 있으며, 더 복잡하고 다양한 시각 생성 작업을 처리하지 못합니다. OmniGen은 다양한 시각 생성 작업을 수행할 수 있는 최초의 범용 모델로, 텍스트-이미지 생성, 이미지 편집, 주제 기반 생성, 가상 피팅, 이미지 디블러링, 인간 포즈 인식 등을 포함하여 넓은 범위의 작업을 수행할 수 있습니다. 이러한 기초 모델을 바탕으로 텍스트 생성까지 확장하는 것이 연구의 다음 단계로 계획되어 있습니다.

### 7.2 Diffusion Model

최근 diffusion 모델의 발전은 놀라운 성과를 거두었으며, Stable Diffusion 시리즈 [56; 52; 13], DALL-E [55], Imagen [26] 등에서 눈에 띄는 성과가 있었습니다. 이러한 모델들은 주로 텍스트-이미지 생성 작업을 위해 설계되었습니다. 시각적 조건 생성 기능을 지원하기 위해 ControlNet [73]과 T2i-Adapter [45]와 같은 접근법이 텍스트-이미지 모델에 추가적인 네트워크를 도입하여 이미지 기반 조건을 수용할 수 있게 했습니다. StyleShot [17]은 출력 이미지의 스타일 기능을 조정하기 위해 스타일 민감 인코더를 도입했습니다. InstructPix2Pix [4]는 추가 입력 채널을 모델에 추가하여 이미지 편집 작업을 처리했습니다. SEED-X [20]과 Kosmos-G [49]는 SD 모델의 CLIP 인코더를 MLLM으로 대체하여 특정 다운스트림 작업의 성능을 개선했습니다. 그러나 이러한 방법들은 대부분 특정 작업에 최적화된 모델로, SD의 기능을 확장하기 위해 모델 아키텍처를 수정합니다. OmniGen은 다양한 이미지 생성 작업을 단일 프레임워크로 통합한 모델로, 멀티태스크 학습을 통해 모델의 기능을 향상시키며 새로운 능력이 발현될 수 있도록 합니다. 

컴퓨터 비전(CV) 작업 통합을 탐구한 일부 연구가 [2; 65; 16; 21] 있지만, 이러한 연구들은 주로 고전적인 비전 작업에 중점을 두며 일반적인 이미지 생성 작업을 지원하지 못합니다. 또한 현재 모델들은 해당 작업에 특화된 모델보다 성능이 떨어져 실제 응용에서 사용되기 어려운 점이 있습니다. 우리의 연구에서 CV 작업을 도입한 것은 모델이 범용적인 지식을 학습하여 이미지 생성 기능을 향상시키고 새로운 능력이 발현될 수 있도록 돕는 데 중요한 역할을 합니다. 예를 들어, 인간 포즈 추정 작업을 도입한 결과, 참조 이미지의 포즈를 기반으로 새로운 이미지를 생성할 수 있는 능력이 모델에 생겼습니다. 현재 우리는 CV 작업에서 최적의 점수를 얻기보다는, 더 복잡하고 다양한 장면을 처리할 수 있도록 그림 그리는 과정을 감독하는 연구 방향을 미래 과제로 남겨두고 있습니다.

---
