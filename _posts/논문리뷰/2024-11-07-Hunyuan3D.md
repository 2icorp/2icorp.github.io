---

title: "[논문리뷰] Hunyuan3D-1.0 Review"  
last_modified_at: 2024-10-17  
categories:  
  - 논문리뷰  
tags:  
  - Hunyuan3D  
  - Text-to-3D  
  - Image-to-3D  
  - Diffusion Models  
  - Consistent Generation  
excerpt: "텍스트와 이미지 기반으로 일관된 고품질 3D 생성을 지원하는 Hunyuan3D-1.0 모델 리뷰"  
use_math: true  
classes: wide  

---

## Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation

Xianghui Yang$^{*}$, Huiwen Shi$^{*}$, Bowen Zhang$^{*}$, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu,  
Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu,  
Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, Chunchao Guo †  
Tencent Hunyuan

Figure 1. 3D asset gallery. 모든 3D 자산은 Hunyuan3D-1.0을 사용하여 텍스트 프롬프트 또는 단일 이미지를 입력으로 생성되었습니다. Hunyuan3D1.0은 텍스트 및 이미지 기반 3D 생성 모두를 지원하는 통합 프레임워크입니다.

## Abstract

3D 생성 모델은 아티스트의 작업 흐름을 크게 개선했지만, 기존의 3D 생성 diffusion 모델은 생성 속도가 느리고 일반화가 부족한 문제가 있습니다. 이를 해결하기 위해, 텍스트 및 이미지 기반 생성 모두를 지원하는 두 단계 접근법 Hunyuan3D-1.0(라이트 버전과 표준 버전)을 제안합니다. 첫 번째 단계에서는 멀티뷰 RGB를 약 4초 안에 효율적으로 생성하는 멀티뷰 diffusion 모델을 사용합니다. 이 멀티뷰 이미지는 다양한 시점에서 3D 자산의 풍부한 세부 정보를 캡처하여 단일 시점 재구성의 부담을 멀티뷰 재구성으로 완화합니다. 두 번째 단계에서는 생성된 멀티뷰 이미지를 약 7초 만에 신속하고 정확하게 재구성하는 피드포워드 재구성 모델을 도입합니다. 이 재구성 네트워크는 멀티뷰 diffusion으로 인해 발생하는 노이즈와 불일치를 처리하며, 조건 이미지로부터 얻은 정보를 활용하여 효율적으로 3D 구조를 복구합니다. 우리의 프레임워크는 텍스트-이미지 모델인 Hunyuan-DiT를 포함하여 텍스트 및 이미지 기반 3D 생성을 모두 지원하는 통합 프레임워크를 만듭니다. 표준 버전은 라이트 버전 및 기존 모델보다 3배 많은 파라미터를 보유하고 있습니다. Hunyuan3D-1.0은 속도와 품질의 균형을 인상적으로 유지하면서 생성 시간을 크게 줄이면서도 생산된 자산의 품질과 다양성을 유지합니다.

## 1. Introduction

3D 생성은 컴퓨터 비전과 컴퓨터 그래픽스 분야에서 오랫동안 매력적이고 활발한 주제였으며, 게임, 영화, 전자상거래 및 로봇 공학 등에서 중요한 응용 프로그램이 있습니다. 고품질 3D 자산을 만드는 것은 아티스트에게 시간 소모적인 과정이므로, 자동 생성을 위한 연구는 오랜 기간 동안 연구자들의 목표였습니다. 초기 연구는 특정 카테고리 내에서 무조건적 생성에 집중하였으며, 이는 3D 표현 및 데이터 제한에 의해 제약되었습니다. 대형 언어 모델(LLMs)의 성공과 이미지 및 비디오 생성의 발전은 장기적인 비전을 실현할 수 있는 길을 열었습니다. 그러나 3D 자산 생성에서 유사한 발전을 이루는 것은 3D 자산의 표현력과 포괄적인 데이터셋의 부족으로 인해 여전히 어려운 과제로 남아 있습니다. 가장 큰 3D 데이터셋인 Objarverse-xl은 단 1천만 개의 자산을 포함하고 있으며, 이는 언어, 이미지 및 비디오 작업에 사용되는 대규모 데이터셋에 비해 매우 적습니다. 2D 생성 모델의 사전 지식을 활용하는 것은 이러한 한계를 극복하기 위한 유망한 접근법을 제시합니다.

2D 생성 모델을 활용하기 위해, 선구적인 연구들은 이 문제를 탐구하여 주목할 만한 발전을 이루었습니다. Poole et al.은 Score Distillation Sampling(SDS)을 이용하여 2D 이미지 diffusion 모델을 통해 3D 표현(Nerf)을 증류했습니다. 이 접근법은 과포화와 시간 소모 문제를 포함하고 있지만, 후속 2D 리프팅 연구에 영감을 주었습니다. 후속 연구들은 샘플링 효율성을 향상시키고, diffusion 모델을 멀티뷰 diffusion 프레임워크로 조정하거나, 샘플링 손실을 일반적인 렌더링 손실로 대체하는 연구들이 진행되었습니다. 그러나 이러한 최적화 기반 방법들은 여전히 시간이 많이 소요되며, 3D 표현 최적화에는 약 5분에서 1시간이 소요됩니다. 반면, 피드포워드 방법들은 수 초 내에 3D 객체를 생성할 수 있지만, 미지의 객체에 대한 일반화가 부족하고 얇고 종이 같은 구조를 생성하지 못하는 경우가 많습니다. 단일 시점 생성 작업을 멀티뷰 이미지 생성 및 희소 시점 재구성으로 분리하는 것은 일반화 문제를 완화하고 SDS에서 최적화 문제를 제거할 수 있는 유망한 경로입니다.

몇몇 연구들은 멀티뷰 생성 및 희소 시점 재구성을 통해 이 문제를 해결하려고 했지만, 이러한 접근법을 통합하여 이들의 결합된 문제를 해결하는 통합 프레임워크는 많지 않습니다. 먼저, 널리 사용되는 멀티뷰 diffusion 모델은 멀티뷰 불일치와 느린 디노이징 과정으로 비판받고 있습니다. 두 번째로, 희소 시점 재구성 모델은 보통 시점 인식 RGB 이미지에만 의존하여 3D 표현을 예측합니다. 이러한 문제들을 개별적으로 해결하는 것은 어려운 과제입니다. 이 하위 작업들을 함께 다루어야 할 필요성을 인식하여, 우리는 Hunyuan3D1.0을 제안합니다. 이는 멀티뷰 diffusion 모델과 희소 시점 재구성 모델의 강점을 통합하여 최상의 경우 약 10초 내에 3D 생성을 달성하며, 일반화와 품질 간의 미묘한 균형을 유지합니다. 첫 번째 단계에서는 멀티뷰 diffusion 모델이 RGB를 생성하여 2D에서 3D로 리프팅을 완료합니다. 우리는 대규모 2D diffusion 모델을 미세 조정하여 멀티뷰 이미지를 생성하고, 모델이 3D 정보를 더 잘 이해하도록 합니다. 또한, 생성된 시점 간의 가시 영역을 최대화하기 위해 생성된 시점에 대해 0-높이 카메라 궤도를 설정합니다. 두 번째 단계에서는 희소 시점 재구성 모델이 불완전하게 일관된 멀티뷰 이미지를 활용하여 기본 3D 모양을 복구합니다. 대다수의 희소 시점 재구성 모델은 포즈가 알려진 RGB 이미지만 사용하지만, 우리는 알려진 시점 포즈가 없는 조건 이미지를 보조 입력으로 추가하여 생성된 멀티뷰 이미지에서 보이지 않는 부분을 보완합니다. 또한, 추가 메모리나 계산 비용 없이 잠재 공간의 세부 정보를 풍부하게 하기 위해 선형 unpatchify 레이어 작업을 사용합니다.

우리의 기여는 다음과 같이 요약됩니다.

- · 텍스트 및 이미지 기반 3D 생성을 모두 지원하는 통합 프레임워크 Hunyuan3D-1.0을 소개합니다.
- · 멀티뷰 생성에서 0-높이 포즈 분포를 설계하여 생성된 시점 간 가시 영역을 최대화합니다.
- · 서로 다른 시점 생성을 위한 제어 가능성과 다양성을 균형 있게 유지하기 위해 시점 인식 classifier-free guidance를 도입합니다.
- · 희소 시점 재구성 과정에서 보조 보기로 보정되지 않은 조건 이미지를 포함하여 생성된 이미지에서 보이지 않는 부분을 보완합니다.

## 2. Related Works

최근 멀티뷰 생성 모델과 sparse-view 재구성 모델의 발전은 이미지-3D 생성의 품질을 크게 향상시켰습니다. 여기서는 관련 연구를 간략히 요약합니다.

Row-wiseAttention Full-pixelAttention

Mix-upAttention

Mix-up Attention Sefl-Att. Row-wiseAttention Row-wise Attention Figure 2. Hunyuan3D-1.0의 개요. 입력 이미지가 주어지면, 먼저 멀티뷰 diffusion 모델을 사용하여 고정된 카메라 포즈에서 6개의 새로운 시점을 합성합니다. 그런 다음 생성된 멀티뷰 이미지를 transformer 기반 sparse-view 대형 재구성 모델에 입력하여 고품질 3D 메시를 재구성합니다. 전체 이미지-3D 생성 프로세스는 약 10초가 소요됩니다.

---

Mix-up Attention Sefl-Att. … Generation Condition Generation2 Generation2 방법은 주로 깊이 추정 [2, 3] 또는 복셀 표현 [5, 17, 33, 38, 45]을 위한 feature matching을 강조합니다. 학습 기반 MVS 방법은 일반적으로 feature matching [10, 18, 29, 46, 67], depth fusion [8, 35], 그리고 멀티뷰 이미지에서의 depth 추론 [14, 61, 63, 66]과 같은 특정 모듈을 학습 가능한 네트워크로 대체합니다. MVS의 명시적 표현과 대조적으로, 최근 신경 접근법은 멀티 레이어 퍼셉트론(MLPs)을 통해 암묵적인 필드를 표현합니다. 이러한 방법들은 주로 복잡한 보정 절차인 구조-이동(Structure-from-Motion) 접근법 [15, 37]을 통해 얻어진 카메라 파라미터 추정에 의존합니다. 하지만 현실적인 시나리오에서, 사전에 추정된 카메라 파라미터의 부정확성은 이러한 알고리즘의 성능에 악영향을 미칠 수 있습니다. 최근 연구들은 카메라 파라미터에 대한 명시적인 지식 없이도 가시 표면의 기하학을 직접 예측하는 방법을 제안했습니다. 우리는 기존 방법이 순수하게 포즈가 알려진 이미지나 포즈가 없는 이미지를 입력으로 가정하는 경우가 많다는 것을 확인했습니다. 본 연구에서는 이러한 격차를 해소하기 위해 교정된 입력과 교정되지 않은 이미지를 모두 고려하여 세부적인 재구성을 수행하고, 희소 시점 재구성 프레임워크를 3D 생성 파이프라인에 더 잘 통합합니다.

## 3. Methods

본 절에서는 우리의 접근 방식인 Hunyuan3D-1.0의 두 단계를 소개합니다. 먼저, Sec. 3.1에서는 2D에서 3D로 전환을 위한 멀티뷰 diffusion 모델을 소개합니다. 두 번째로, Sec. 3.2에서는 sparse-view 재구성 프레임워크 내에서 포즈가 알려진 이미지와 알려지지 않은 이미지의 융합과 초해상도 레이어에 대해 논의합니다.

### 3.1. Multi-view Diffusion Model

2D 생성에서 diffusion 모델의 성공을 목격하면서, novel-view 생성 모델에서도 diffusion 모델의 잠재력이 탐구되었습니다. 대부분의 novel-view [23, 53] 또는 멀티뷰 [25, 40, 47, 48] 생성 모델은 대량의 데이터로 학습된 diffusion 모델의 일반화 능력을 활용합니다. 우리는 3배 더 많은 파라미터를 가진 대형 모델을 학습하여 이 모델을 확장했습니다.

**Multi-view Generation**. 우리는 멀티뷰 이미지를 그리드로 구성하여 동시에 멀티뷰 이미지를 생성합니다. 이를 위해 Zero-1-to-3++ [39]을 따르며, 모델을 3배 더 큰 모델로 확장합니다. 참조 주의를 Zero-1-to-3++ [39]에서 사용한 것과 같이 활용하여 diffusion 모델이 참조 이미지와 유사한 의미적 내용과 텍스처를 공유하는 이미지를 생성하도록 합니다. 이 과정에는 디노이징 중에 조건 이미지로부터 self-attention key와 value 행렬을 추가하는 것이 포함됩니다. Zero-1-to-3++의 렌더링 설정과 달리, 목표 이미지는 0도 고도와 6개의 방위각 (0°, 60°, 120°, 180°, 240°, 300°)로 구성되며 흰색 배경으로 렌더링됩니다. 목표 이미지는 라이트 모델의 경우 960 × 640, 표준 모델의 경우 1536 × 1024 크기로 3 × 2 그리드로 배열됩니다.

**Adaptive Classifier-free Guidance**. Classifier-free guidance(CFG)는 diffusion 모델에서 제어 가능성과 다양성을 균형 있게 유지하는 샘플링 기법으로 널리 사용됩니다. 멀티뷰 생성에서는 작은 CFG가 세부적인 텍스처를 합성하는 데 도움이 되지만, 불필요한 아티팩트를 생성할 수 있습니다. 반면에 큰 CFG는 객체의 기하학적 구조를 우수하게 표현하나 텍스처 품질을 저하시킵니다. 다양한 시점에서 CFG 값의 성능이 상이함을 감안하여 우리는 시점 및 시간 단계에 따라 다른 CFG 값을 설정하는 Adaptive Classifier-Free Guidance 스케줄을 제안합니다. 직관적으로, 전면 시점에서는 초기 디노이징 단계에서 높은 CFG 스케일을 설정하고, 디노이징이 진행됨에 따라 CFG 스케일을 점차 감소시킵니다.

\[
w_{t} = 2 + 16 \times ( t / 1000) ^5
\]

다른 시점에서는 스케일을 적용하여

\[
w_{t, v} = w_{t} \times \tau_{v}
\]

여기서 \(\tau_{v}\)는 시점 간 거리에 따라 0.5에서 1 사이로 설정됩니다. 이를 통해 시점별 디노이징 단계에서의 균형을 유지하며 멀티뷰 생성 품질을 높입니다.

### 3.2. Sparse-view Reconstruction Model

이 절에서는 sparse-view 재구성 모델을 설명합니다. 이는 멀티뷰 diffusion 모델에서 생성된 멀티뷰 이미지를 사용하여 3D 모양을 2초 안에 복구하는 transformer 기반 접근 방식입니다. 대형 재구성 모델들이 일반적으로 1개 또는 3개의 RGB 이미지에 의존하는 반면, 우리는 교정된 입력과 교정되지 않은 입력을 결합하여 고품질의 3D 재구성을 실현했습니다. 이 접근 방식은 기존 방법의 한계를 극복하고, 실질적인 3D 생성 작업에 적합한 견고한 솔루션을 제공합니다.

**Hybrid Inputs**. 우리의 sparse-view 재구성 모델은 교정된 이미지와 교정되지 않은 이미지의 조합을 활용하여 재구성을 수행합니다. 교정된 이미지는 훈련 단계에서 미리 정의된 카메라 임베딩과 함께 제공됩니다. 멀티뷰 생성이 0도 고도 궤도로 제한되기 때문에 모델은 상단 또는 하단 시점에서 정보를 캡처하는 데 어려움이 있습니다. 이 한계를 극복하기 위해 우리는 교정되지 않은 조건 이미지를 재구성 과정에 통합하여, 이러한 불확실성을 최소화하고 희소한 시점에서도 정확한 3D 모양을 복원할 수 있습니다.

**Super-resolution**. Transformer 기반 재구성에서 높은 해상도의 특징은 3D 형상의 세부사항을 더욱 잘 인코딩할 수 있습니다. 그러나 대부분의 기존 작업들은 저해상도 triplane을 사용하며, 이는 해상도의 제약으로 인해 아티팩트가 발생합니다. 이를 해결하기 위해 우리는 triplane 초해상도 모듈을 제안하여 입력 크기에 따라 선형 복잡성을 유지하면서 고해상도의 세부사항을 캡처할 수 있도록 했습니다.

**3D Representation**. 대부분의 3D 생성 모델이 NeRF나 Gaussian Splatting과 같은 암묵적 표현으로 끝나는 반면, 우리는 명시적 표현이 실용적인 응용 프로그램에서 더 유용하다고 주장합니다. 따라서 우리는 NeuS의 Signed Distance Function(SDF)을 사용하여 암묵적 표현을 통해 형상을 나타내고, 이를 marching cube 알고리즘으로 명시적 메쉬로 변환하여 다양한 응용 프로그램에 직접 사용 가능한 최종 출력을 생성합니다.

## 4. Implementation

**Training datasets**. 우리는 멀티뷰 diffusion 모델과 sparse-view 재구성 모델을 내부 데이터셋을 사용하여 학습했습니다. 학습 데이터의 품질과 관련성을 보장하기 위해, 복잡한 장면이 포함된 3D 데이터를 필터링하고 모든 3D 객체를 단위 구에 맞추어 스케일링했습니다.

**Training details**. 멀티뷰 diffusion 모델과 sparse-view 재구성 모델을 별도로 학습했습니다. 멀티뷰 diffusion 모델의 라이트 버전은 SD-2.1을 백본으로 채택했으며, 표준 버전은 SD-XL을 백본으로 사용했습니다.

**Evaluation**. 모델을 기존 접근법과 비교하기 위해 GSO 및 OmniObject3D의 두 공개 데이터셋에서 평가했습니다. 암묵적 3D 표현을 메쉬로 변환하기 위해 Marching Cubes 알고리즘을 사용하여 등고면을 추출했습니다. 여기에서 10,000 포인트를 샘플링하여 Chamfer Distance(CD) 및 F-score(FS)를 계산했습니다. 

## 5. Results

우리는 Hunyuan3D1.0과 기존 최첨단 방법을 두 가지 다른 데이터셋을 사용하여 3D 재구성 메트릭으로 정량적 및 정성적으로 비교했습니다. Table 1과 Table 2에 제시된 것처럼, 우리의 표준 버전 Hunyuan3D-1.0은

 모든 기준선을 능가하는 성능을 보였습니다. 특히, 사용자 선호도 측면에서 Hunyuan3D-1.0이 5가지 메트릭에서 가장 높은 평가를 받았습니다.

## 6. Ablation Studies

우리는 adaptive CFG 및 hybrid inputs의 효과를 검증하기 위해 실험을 진행했습니다.

**Adaptive CFG**. Adaptive Classifier-Free Guidance(CFG)가 멀티뷰 이미지 생성에서 기존의 고정 CFG보다 더 나은 성능을 보여주었으며, 이를 통해 보다 자연스럽고 일관된 멀티뷰 이미지를 생성할 수 있음을 확인했습니다.

**Hybrid Inputs**. Hybrid input 기법은 3D 형상의 보이지 않는 부분을 재구성하는 데 크게 기여하였으며, 이로 인해 보다 완전하고 정확한 3D 모양을 생성할 수 있었습니다.

## 7. Conclusion

본 연구는 고품질의 3D 형상을 생성할 수 있는 두 단계의 3D 생성 파이프라인인 Hunyuan3D-1.0을 소개했습니다. 이 파이프라인은 멀티뷰 이미지 생성을 통해 텍스처와 기하학적 세부사항이 풍부한 멀티뷰 이미지를 생성하고, 피드포워드 sparse-view 재구성 모델을 통해 명시적 표현으로 3D 모양을 복구합니다. 제안된 방법은 adaptive classifier-free guidance와 hybrid inputs를 통해 unseen 부분을 보완하는 데 효과적이었으며, 가벼운 초해상도 모듈을 통해 세부 표현을 강화하여 3D 생성의 효율성과 품질을 크게 향상시켰습니다.

