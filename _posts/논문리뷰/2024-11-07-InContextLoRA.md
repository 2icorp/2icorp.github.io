---

title: "[논문리뷰] In-Context LoRA for Diffusion Transformers Review"  
last_modified_at: 2024-10-17  
categories:  
  - 논문리뷰  
tags:  
  - In-Context LoRA  
  - Text-to-Image  
  - Diffusion Transformers  
  - Consistent Generation  
  - Training-Free  
excerpt: "훈련 없이 다양한 생성 과제에 적응 가능한 In-Context LoRA 모델 리뷰"  
use_math: true  
classes: wide  

---

## IN-CONTEXT LORA FOR DIFFUSION TRANSFORMERS

Lianghua Huang

Wei Wang

Zhi-Fan Wu

Yupeng Shi

Huanzhang Dou

Chen Liang

Yutong Feng

Yu Liu

Jingren Zhou

Tongyi Lab *

## ABSTRACT

최근 연구 [Huang et al., 2024]에서는 여러 이미지 간의 attention 토큰을 단순히 결합하여 diffusion transformers (DiTs)를 사용해 task-agnostic 이미지 생성에 대해 연구를 진행했습니다. 그러나 상당한 컴퓨팅 자원에도 불구하고 생성된 이미지의 정밀도는 여전히 최적화되지 않은 상태입니다. 본 연구에서는 텍스트-이미지 DiT들이 근본적으로 in-context 생성 기능을 가지고 있으며, 이를 활성화하기 위해 최소한의 조정만 필요하다는 가설을 세우고 이 프레임워크를 재평가하고 간소화합니다. 다양한 과제 실험을 통해 기존 텍스트-이미지 DiTs가 어떠한 튜닝 없이 효과적으로 in-context 생성을 수행할 수 있음을 질적으로 입증합니다. 이를 기반으로 DiT의 in-context 능력을 활용하는 매우 간단한 파이프라인을 제안합니다. 이 방법은 이미지 대신 이미지의 attention 토큰을 결합하고, 여러 이미지를 공동 캡션화하며, 대규모 데이터 세트를 사용한 전체 파라미터 튜닝 대신 소규모 데이터 세트(예: 20 ∼ 100 샘플)로 task-specific LoRA 튜닝을 수행합니다. 우리는 이 모델을 In-Context LoRA (IC-LoRA)라고 명명합니다. 이 접근법은 기존 DiT 모델의 수정이 전혀 필요하지 않으며, 학습 데이터의 변경만을 필요로 합니다. 이 파이프라인은 보다 높은 정밀도의 이미지 세트를 생성하며, 프롬프트에 더 잘 맞추도록 합니다. 튜닝 데이터는 task-specific하나, 전체 아키텍처와 파이프라인은 task-agnostic하여 커뮤니티에 강력한 도구를 제공하고, 제품 수준의 task-agnostic 생성 시스템에 대한 추가 연구에 중요한 통찰을 제공합니다. 코드, 데이터 및 모델은 https://github.com/ali-vilab/In-Context-LoRA 에서 공개합니다.

키워드: In-context LoRA · Diffusion transformers · Image generation

## 1 Introduction

텍스트-이미지 모델의 도입으로 시각적 콘텐츠 생성 분야는 큰 진전을 이루었으며, 텍스트 설명에서 고정밀 이미지 생성이 가능해졌습니다 [Ramesh et al., 2021, 2022, Esser et al., 2021, Rombach et al., 2022, Saharia et al., 2022a, Betker et al., 2023, Podell et al., 2023, Esser et al., 2024, Baldridge et al., 2024, Labs, 2024]. 다양한 이미지 속성에 대한 세밀한 조정이 가능하도록 제어 기능이 향상된 다양한 방법들이 제안되었습니다 [Zhang et al., 2023, Ye et al., 2023, Huang et al., 2023, Ruiz et al., 2023, Wang et al., 2024a, Hertz et al., 2024]. 하지만 텍스트-이미지 모델을 복잡한 내적 관계를 가진 일관된 이미지 세트를 생성하는 다양한 생성 과제에 적응시키는 것은 여전히 풀리지 않은 도전 과제입니다. 본 연구에서는 다양한 생성 과제에 텍스트-이미지 모델을 적응시키기 위한 task-agnostic 프레임워크를 소개하여 다용도와 제어 가능한 이미지 생성을 위한 보편적인 솔루션을 제공하는 것을 목표로 합니다.

---

---

Figure 1: In-Context LoRA Generation Examples. 세 가지 과제: 인물 사진 촬영, 폰트 디자인, 그리고 홈 데코레이션. 각 과제에 대해 In-Context LoRA 모델이 특정 과제를 위해 튜닝된 상태에서 단일 diffusion 프로세스를 통해 네 개의 이미지를 동시에 생성합니다.

<!-- 이미지 설명 -->

이러한 예시들은 In-Context LoRA가 다양한 과제에서 이미지를 생성하는 방식을 보여줍니다. 예를 들어, 젊은 예술가의 창의적인 작업 과정을 나타내는 이미지 세트, 빈티지 테마의 비주얼에서 복고풍 폰트가 창의적으로 적용된 장면, 그리고 다채롭고 개성 넘치는 홈 데코레이션 장면을 통해 각기 다른 작업 흐름을 시각화합니다.

---

## 2 Related Work

### 2.1 Task-Specific Image Generation

텍스트-이미지 모델은 복잡한 텍스트 프롬프트로부터 고정밀 이미지를 생성하는 데 상당한 성과를 거두었습니다 [Ramesh et al., 2021, 2022, Esser et al., 2021, Rombach et al., 2022, Saharia et al., 2022a, Betker et al., 2023, Podell et al., 2023, Chen et al., 2023, Esser et al., 2024, Baldridge et al., 2024, Labs, 2024]. 하지만 이러한 모델들은 생성된 이미지의 특정 속성을 세밀하게 제어하는 능력이 부족한 경우가 많습니다. 이를 해결하기 위해 레이아웃 [Zheng et al., 2023, Huang et al., 2023], 포즈 [Zhang et al., 2023], 아이덴티티 [Huang et al., 2023, Ye et al., 2023, Li et al., 2024], 컬러 팔레트 [Huang et al., 2023], 스타일 [Hertz et al., 2024, Huang et al., 2023], 영역 [Meng et al., 2021, Lugmayr et al., 2022, Xie et al., 2022, Huang et al., 2023], 왜곡된 이미지 [Saharia et al., 2022b, Kawar et al., 2022, Xia et al., 2023, Li et al., 2023] 및 조명 조건 [Zhang et al., 2023]에 대한 제어를 향상시키기 위한 여러 연구가 이루어졌습니다. 일부 연구는 다중 이미지 생성을 지원하며, 이는 본 연구와 유사한 접근입니다 [Zhou et al., 2024a, Liu et al., 2024b, Yang et al., 2024, Chern et al., 2024, Huang et al., 2024].

---

### 2.2 Task-Agnostic Image Generation

task-specific 모델의 한계를 극복하기 위해, 최근 연구는 단일 아키텍처 내에서 다중 제어 가능한 이미지 생성 과제를 지원하는 task-agnostic 프레임워크를 개발하는 데 주력하고 있습니다 [Ge et al., 2023, Zhou et al., 2024b, Sheynin et al., 2024, Sun et al., 2024, Wang et al., 2024b]. 예를 들어, Emu Edit [Sheynin et al., 2024]는 다양한 이미지 편집 기능을 통합하고 있으며, Emu2 [Sun et al., 2024], Emu3 [Wang et al., 2024b], TransFusion [Zhou et al., 2024b], Show-o [Xie et al., 2024], 및 OmniGen [Xiao et al., 2024]과 같은 모델들은 단일 프레임워크에서 절차적 드로잉부터 피사체 기반 생성까지 다양한 과제를 수행합니다. Emu3는 텍스트, 이미지 및 비디오 생성을 하나의 프레임워크로 확장하여 더 다양한 기능을 지원합니다. 이러한 연구들은 통합 또는 task-agnostic 생성의 상당한 진보를 보여줍니다.

본 연구는 기존의 텍스트-이미지 아키텍처가 본래 in-context 기능을 내포하고 있다고 제안하며, 새로운 아키텍처를 개발할 필요 없이 소량의 추가 데이터와 컴퓨팅 자원으로 고품질의 생성이 가능함을 보여줍니다. 우리의 접근은 효율성을 증대시킬 뿐만 아니라 다양한 과제에 걸쳐 우수한 생성 품질을 제공합니다.

---

## 3 Method

### 3.1 Problem Formulation

Group Diffusion Transformers [Huang et al., 2024] 접근을 따르며 대부분의 이미지 생성 과제를 n ≥ 1개의 이미지 세트를 생성하고 m ≥ 0개의 이미지 및 (n + m) 텍스트 프롬프트에 조건화하는 과제로 정의합니다. 이는 이미지 번역, 스타일 전이, 포즈 전이, 피사체 기반 생성과 같은 다양한 학술 과제와, 그림책 제작, 폰트 디자인과 전이, 스토리보드 생성 등의 실용적인 응용을 포함합니다 [Huang et al., 2024].

---

### 3.2 Group Diffusion Transformers

기본 프레임워크는 Group Diffusion Transformers (GDT) [Huang et al., 2024]입니다. GDT에서는 이미지 세트를 동시에 생성하기 위해 각 Transformer self-attention 블록에서 여러 이미지 간의 attention 토큰을 결합합니다. 텍스트 조건화는 각 이미지가 해당 텍스트 임베딩을 참조하도록 하여 다른 이미지의 콘텐츠와 관련된 텍스트 지침을 사용할 수 있도록 합니다.

GDT는 수십만 개의 이미지 세트를 학습하여 zero-shot 방식으로 다양한 과제를 일반화할 수 있습니다.

---

### 3.3 In-Context LoRA

GDT가 zero-shot 과제 적응성을 보여주기는 하지만, 생성 품질이 낮아 기본 텍스트-이미지 모델과 비교하여 성능이 떨어지는 경우가 많습니다. 우리는 이 프레임워크를 개선하기 위한 방법을 제안합니다.

첫 번째 가정은 기본 텍스트-이미지 모델이 다양한 과제를 위해 일정 수준의 in-context 생성 능력을 본래 갖추고 있다는 점입니다. Figure 3의 결과에서 볼 수 있듯이, 모델은 다양한 과제에서 다중 이미지를 효과적으로 생성하고 있습니다. 이를 통해 대규모 데이터 세트를 사용한 광범위한 학습이 필수적이지 않다는 것을 알 수 있으며, 고품질 이미지 세트를 통해 모델의 in-context 기능을 활성화할 수 있습니다.

또 다른 중요한 관찰은 텍스트-이미지 모델이 여러 패널의 설명을 포함한 단일 프롬프트로 일관된 다중 패널 이미지를 생성할 수 있다는 점입니다(Figure 3, Appendix A 참조). 따라서 각 이미지가 해당 텍스트 토큰에만 주의를 기울이는 기존의 방식 대신, 통합된 이미지 프롬프트를 사용할 수 있으며, 이를 통해 원본 텍스트-이미지 아키텍처를 구조적 수정 없이 재사용할 수 있습니다.

최종적으로, 우리 프레임워크는 학습 시 여러 이미지를 하나의 큰 이미지로 병합하고, 이를 포괄하는 설명과 각 패널에 대한 명확한 지침을 포함한 통합된 프롬프트를 사용하여 이미지 세트를 동시에 생성합니다. 생성된 이미지 세트는 개별 패널로 분할됩니다. 텍스트-이미지 모델이 이미 in-context 기능을 보여주고 있기 때문에 모델 전체를 튜닝하지 않고, 소량의 고품질 데이터를 통해 Low-Rank Adaptation (LoRA) 기법을 사용하여 이 기능을 활성화하고 강화할 수 있습니다.

추가 이미지를 조건화하기 위해, 우리는 SDEdit [Meng et al., 2021]을 사용하여 마스크된 이미지 부분을 채우는 방식으로 조건 이미지를 기반으로 한 인페인팅을 수행합니다.

비록 간단하지만, 우리의 방법은 다양한 과제에 고품질로 적응할 수 있습니다. Figures 1과 2는 여러 과제에 대한 예시 출력을 보여주고, Figures 4-12는 각 과제별로 구체적인 사례를, Figures 13과 14는 이미지 조건 생성 결과를 제시합니다. 비록 과제별로 특정한 튜닝 데이터가 필요하지만, 전체적인 프레임워크와 파이프라인은 task-agnostic하며, 원본 모델 아키텍처의 수정 없이 다양한 과제에 적응할 수 있습니다. 이러한 데이터 요구 사항의 최소화와 광범위한 적용 가능성은 생성 커뮤니티, 디자이너 및 아티스트에게 강력한 도구를 제공합니다. 완전히 통합된 생성 시스템을 개발하는 것은 여전히 열려있는 과제이며, 이는 미래 연구로 남깁니다. 추가 연구를 지원하기 위해, 본 프로젝트의 데이터, 모델 및 학습 설정은 프로젝트 페이지에서 공개됩니다$^{2}$.

---

## 4 Experiments

### 4.1 Implementation Details

우리의 접근법은 FLUX.1-dev 텍스트-이미지 모델 [Labs, 2024]을 기반으로 하며, 과제별로 In-Context LoRA를 훈련합니다. 스토리보드 생성, 폰트 디자인, 인물 사진 촬영, 비주얼 아이덴티티 디자인, 홈 데코레이션, 비주얼 효과, 인물 일러스트레이션, 파워포인트 템플릿 디자인 등 다양한 실용적 과제를 선택했습니다. 각 과제에 대해 인터넷에서 20~100개의 고품질 이미지 세트를 수집하고, 각 이미지 세트는 하나의 합성 이미지로 병합됩니다. 이 이미지들에 대한 캡션은 Multi-modal Large Language Models (MLLMs)을 사용하여 전체적인 요약과 각 이미지에 대한 상세 설명을 포함하여 생성했습니다. 학습은 A100 GPU 한 대에서 5,000 스텝 동안 배치 크기 4로 수행되었으며, LoRA 랭크는 16입니다. 추론 시, FLUX.1-dev의 디스틸레이션 가이드 스케일에 맞춘 20개의 샘플링 스텝을 사용하며 가이드 스케일은 3.5입니다. 이미지 조건 생성을 위해 SDEdit을 사용하여 생성할 이미지를 마스킹하여 주변 이미지 기반으로 인페인팅을 수행합니다.

### 4.2 Results

다양한 과제에서 모델의 다재다능함과 품질을 보여주는 정성적 결과를 제시합니다. 과제의 다양성을 고려하여 통합된 정량적 벤치마크와 평가를 향후 연구 과제로 남깁니다.

#### 4.2.1 Reference-Free Image-Set Generation

이 설정에서는 텍스트 프롬프트만을 사용하여 이미지 세트를 생성하며, 추가적인 이미지 입력은 필요하지 않습니다. 다양한 과제에서의 예시는 Figures 4-12에 제시되어 있으며, 우리 방법이 여러 이미지 세트 생성 과제에서 고품질 결과를 달성함을 보여줍니다.

#### 4.2.2 Reference-Based Image-Set Generation

이 설정에서는 텍스트 프롬프트와 최소한 하나 이상의 참조 이미지를 사용하여 이미지 세트를 생성합니다. SDEdit을 통해 특정 이미지를 마스킹하여 남아 있는 이미지에 기반한 인페인팅을 수행합니다. 이미지 조건 생성 결과는 Figure 13에, 일반적인 실패 사례는 Figure 14에 제시되어 있습니다. 다양한 과제에서 효과적으로 작동하지만, 이미지 간의 시각적 일관성은 텍스트 조건 생성과 비교하여 다소 낮을 수 있습니다. 이는 마스킹된 이미지와 마스킹되지 않은 이미지 간의 단방향 의존성으로 인한 결과로 추정되며, 텍스트 기반 생성은 이미지들 간의 양방향 의존성을 가능하게 하여 조건과 출력 간의 상호 조정이 가능합니다. 이는 향후 연구로 트레이닝이 가능한 인페인팅 방법을 도입할 가능성을 제시합니다.

---

---

## 5 Discussion

In-Context LoRA 프레임워크는 기존의 텍스트-이미지 모델을 다양한 생성 과제에 적응시키는 데 있어 흥미로운 가능성을 제공합니다. 여기에서는 본 연구의 주요한 발견과 그 의미를 논의하며, 연구에서 직면한 한계점과 향후 연구 방향에 대해 다룹니다.

### 5.1 Inherent In-Context Capabilities of Text-to-Image Models

본 연구에서 확인한 중요한 결과 중 하나는 텍스트-이미지 모델이 기본적으로 in-context 생성 능력을 지니고 있다는 점입니다. 즉, 모델은 추가적인 구조적 변경 없이 여러 이미지를 동시에 생성하고, 프롬프트 간의 상호 관계를 이해할 수 있습니다. 이는 텍스트-이미지 모델이 일반적인 AI 생성 환경에서 task-agnostic하게 활용될 수 있는 잠재력을 나타냅니다. 기존 연구에서는 주로 아키텍처 수정이나 대규모 학습을 통해 새로운 기능을 추가하는 방식이었으나, 본 연구에서는 소량의 데이터와 제한된 자원만으로도 유사한 성과를 낼 수 있음을 보였습니다.

### 5.2 Limitations and Future Directions

본 연구는 효과적인 결과를 도출했지만, 몇 가지 한계가 있습니다. 첫째, 우리의 프레임워크는 특정 과제에 맞는 튜닝 데이터가 필요하므로 완전히 보편적이지는 않습니다. 다양한 과제에서 높은 품질의 출력을 보장하려면 여전히 task-specific 튜닝이 필요할 수 있습니다. 둘째, SDEdit을 통한 인페인팅 과정에서는 이미지 간의 양방향 의존성이 부족하여, 텍스트-기반 생성과 비교해 시각적 일관성이 다소 낮을 수 있습니다. 향후 연구에서는 트레이닝 가능한 인페인팅 방법을 도입하여 이 문제를 개선할 가능성이 있습니다.

또한, 단일 프롬프트로 여러 이미지의 복잡한 상관 관계를 효율적으로 표현하기 위한 프롬프트 최적화에 대한 추가 연구가 필요합니다. 프롬프트 설계가 생성 품질에 중요한 영향을 미치기 때문에, 프롬프트 개선을 통해 모델의 성능을 더욱 향상시킬 수 있을 것입니다. 마지막으로, 비디오와 같은 동적 미디어 생성으로의 확장이 연구되어야 합니다. 텍스트-이미지 모델이 보유한 in-context 기능이 비디오 생성에서도 유사하게 작용할 수 있는지 탐구하는 것이 흥미로울 것입니다.

---

## 6 Conclusion

본 연구에서는 In-Context LoRA (IC-LoRA)라는 새로운 프레임워크를 제안하여, 기존의 텍스트-이미지 모델을 다양한 생성 과제에 적응시키는 간단하면서도 효과적인 방법을 개발했습니다. 우리의 접근법은 모델 아키텍처에 전혀 변경을 가하지 않고, 소량의 고품질 데이터만으로 task-specific 튜닝을 가능하게 하며, 이를 통해 다중 이미지 생성 과제에서 높은 품질의 결과를 도출할 수 있음을 보여주었습니다. 또한, 텍스트-이미지 모델이 본래 in-context 생성 능력을 내포하고 있음을 확인함으로써, 추가적인 구조적 변경 없이 이러한 능력을 활용할 수 있는 방법을 제시했습니다.

IC-LoRA는 효율적이며 강력한 task-agnostic 생성 시스템으로서, 생성 커뮤니티와 디자이너, 아티스트에게 큰 가능성을 제공합니다. 향후 연구는 프롬프트 설계 최적화, 트레이닝 가능한 인페인팅 방법 개발, 그리고 비디오 생성으로의 확장을 통해 IC-LoRA의 성능을 더욱 강화하는 방향으로 진행될 수 있을 것입니다. 우리는 IC-LoRA가 향후 연구와 응용 개발에 중요한 역할을 할 것이라고 기대하며, 연구의 지속적인 발전을 위해 코드, 데이터 및 학습 설정을 공개합니다.

---

## References

- 본 연구에서 참조한 참고 문헌들을 포함합니다: Ramesh et al., 2021; Esser et al., 2021; Rombach et al., 2022; Saharia et al., 2022a 등.

---
